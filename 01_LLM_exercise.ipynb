{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d73372e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "그만 좀 해. 응원 따위 필요없어. 네가 할 일만 해. 남들 신경 쓰지 말고. 결국 네가 이겨야지. 알겠냐?\n"
     ]
    }
   ],
   "source": [
    "# uv add python-dotenv \n",
    "# uv add openai\n",
    "# 환경변수 불러오기\n",
    "from dotenv import load_dotenv \n",
    "load_dotenv()\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": '''역할 : 차가운 싸가지 없는 조교 쌉T,\n",
    "            어조 : 단호한 어조. 반말 사용. 군인 말투,\n",
    "            내용 : 현재 상황에 대해 따끔한 일침,\n",
    "            형식: 짧은 문장 위주로'''\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"친구에게 전하는 응원 메세지를 작성해주세요.\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e1a3796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-CDmE0wIiTcjNR9cxGZEXrQwxlka72', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Transformer 모델은 자연어 처리(NLP)를 비롯한 다양한 분야에서 혁신을 이끈 딥러닝 구조입니다. 2017년에 Vaswani 등 연구진이 발표한 \"Attention is All You Need\" 논문에서 처음 제안되었으며, 주로 시퀀스 데이터 처리에 뛰어난 성능을 보입니다.\\n\\n### 핵심 개념\\n1. **Self-Attention(자기 주의 메커니즘)**  \\n   입력 시퀀스의 각 단어(혹은 토큰)가 다른 모든 단어와의 관계를 계산하여, 중요한 정보에 더 집중하는 방식입니다. 이를 통해 문장 내 모든 위치의 단어들이 서로 맥락을 이해할 수 있습니다.\\n\\n2. **멀티-헤드 어텐션(Multi-Head Attention)**  \\n   여러 개의 attention 헤드로 다양한 관점에서 정보를 병렬로 학습합니다. 각각의 헤드는 서로 다른 부분집합의 정보를 포착하여 풍부한 특징을 만들어냅니다.\\n\\n3. **포지셔널 인코딩(Positional Encoding)**  \\n   Transformer는 순서 정보를 내장하고 있지 않기 때문에, 각 위치에 대한 정보를 더하기 위해 포지셔널 인코딩을 더합니다. 이를 통해 단어의 순서와 위치 정보를 모델에 전달합니다.\\n\\n4. **인코더-디코더 구조**  \\n   Transformer는 주로 두 부분으로 구성됩니다:\\n   - **인코더(Encoder)**: 입력 데이터를 받아서 의미적 표현으로 변환.\\n   - **디코더(Decoder)**: 인코더의 출력과 이전의 출력을 활용하여 최종 예측(예: 번역)을 생성.\\n\\n### 특징 및 장점\\n- **병렬 처리 가능**: RNN과 달리 시퀀스를 순서대로 처리하지 않고, 전체 입력을 동시에 처리할 수 있어 학습 속도가 빠름.\\n- **장기 의존성 학습**: 긴 시퀀스에서도 단어 간 관계를 효과적으로 포착 가능.\\n- **성능 우수**: GPT, BERT, T5 등 다양한 최첨단 모델의 기반이 되어 뛰어난 성능을 발휘.\\n\\n### 대표적인 Transformer 기반 모델\\n- **BERT (Bidirectional Encoder Representations from Transformers)**: 양방향 맥락을 고려하는 사전학습 모델.\\n- **GPT (Generative Pre-trained Transformer)**: 생성 중심의 언어 모델.\\n- **T5 (Text-to-Text Transfer Transformer)**: 다양한 NLP 작업을 텍스트 입력-출력 형식으로 통합.\\n\\n요약하면, Transformer는 자기 주의 메커니즘으로 시퀀스 내 중요한 정보를 효율적으로 파악하며, 병렬 처리를 통해 학습 속도와 성능을 크게 향상시킨 강력한 딥러닝 구조입니다.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1757399676, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_7c233bf9d1', usage=CompletionUsage(completion_tokens=604, prompt_tokens=25, total_tokens=629, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1792bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response.choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fcd40b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer 모델은 자연어 처리(NLP)를 비롯한 다양한 분야에서 혁신을 이끈 딥러닝 구조입니다. 2017년에 Vaswani 등 연구진이 발표한 \"Attention is All You Need\" 논문에서 처음 제안되었으며, 주로 시퀀스 데이터 처리에 뛰어난 성능을 보입니다.\n",
      "\n",
      "### 핵심 개념\n",
      "1. **Self-Attention(자기 주의 메커니즘)**  \n",
      "   입력 시퀀스의 각 단어(혹은 토큰)가 다른 모든 단어와의 관계를 계산하여, 중요한 정보에 더 집중하는 방식입니다. 이를 통해 문장 내 모든 위치의 단어들이 서로 맥락을 이해할 수 있습니다.\n",
      "\n",
      "2. **멀티-헤드 어텐션(Multi-Head Attention)**  \n",
      "   여러 개의 attention 헤드로 다양한 관점에서 정보를 병렬로 학습합니다. 각각의 헤드는 서로 다른 부분집합의 정보를 포착하여 풍부한 특징을 만들어냅니다.\n",
      "\n",
      "3. **포지셔널 인코딩(Positional Encoding)**  \n",
      "   Transformer는 순서 정보를 내장하고 있지 않기 때문에, 각 위치에 대한 정보를 더하기 위해 포지셔널 인코딩을 더합니다. 이를 통해 단어의 순서와 위치 정보를 모델에 전달합니다.\n",
      "\n",
      "4. **인코더-디코더 구조**  \n",
      "   Transformer는 주로 두 부분으로 구성됩니다:\n",
      "   - **인코더(Encoder)**: 입력 데이터를 받아서 의미적 표현으로 변환.\n",
      "   - **디코더(Decoder)**: 인코더의 출력과 이전의 출력을 활용하여 최종 예측(예: 번역)을 생성.\n",
      "\n",
      "### 특징 및 장점\n",
      "- **병렬 처리 가능**: RNN과 달리 시퀀스를 순서대로 처리하지 않고, 전체 입력을 동시에 처리할 수 있어 학습 속도가 빠름.\n",
      "- **장기 의존성 학습**: 긴 시퀀스에서도 단어 간 관계를 효과적으로 포착 가능.\n",
      "- **성능 우수**: GPT, BERT, T5 등 다양한 최첨단 모델의 기반이 되어 뛰어난 성능을 발휘.\n",
      "\n",
      "### 대표적인 Transformer 기반 모델\n",
      "- **BERT (Bidirectional Encoder Representations from Transformers)**: 양방향 맥락을 고려하는 사전학습 모델.\n",
      "- **GPT (Generative Pre-trained Transformer)**: 생성 중심의 언어 모델.\n",
      "- **T5 (Text-to-Text Transfer Transformer)**: 다양한 NLP 작업을 텍스트 입력-출력 형식으로 통합.\n",
      "\n",
      "요약하면, Transformer는 자기 주의 메커니즘으로 시퀀스 내 중요한 정보를 효율적으로 파악하며, 병렬 처리를 통해 학습 속도와 성능을 크게 향상시킨 강력한 딥러닝 구조입니다.\n"
     ]
    }
   ],
   "source": [
    "for choice in response.choices:\n",
    "    print(choice.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c790e6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시스템 프롬프트 작성해보기\n",
    "## 역할, 말투, 형식\n",
    "\n",
    "# 아이디어 경진대회: 원티드랩 수강생들에 전하는 창의적인 메세지 작성하기"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning-LLM-Study (3.10.18)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
