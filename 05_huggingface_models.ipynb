{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be4c6e40",
   "metadata": {},
   "source": [
    "# 모델 불러오고 추론하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d82e0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uv add huggingface_hub\n",
    "# 환경변수 불러오기\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "login(token=os.getenv(\"HF_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "eca6c772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu126\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a41d0f",
   "metadata": {},
   "source": [
    "## 1) gemma-3-1b-it 모델 불러오기 (instruction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993e0000",
   "metadata": {},
   "source": [
    "### pipeline으로 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19af22fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=\"google/gemma-3-1b-it\", \n",
    "    device=device, \n",
    "    dtype=torch.bfloat16)\n",
    "\n",
    "messages = [\n",
    "    \n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"친절하게 말하세요\"},]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"오늘 하루를 응원해주세요\"},] \n",
    "            # 여러 형식의 데이터를 받을 수 있는 멀티 모달이기 때문에 type 키가 있음\n",
    "        },\n",
    "]\n",
    "\n",
    "output = pipe(messages, max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7929e216",
   "metadata": {},
   "source": [
    "- Input:\n",
    "   - Text string, such as a question, a prompt, or a document to be summarized\n",
    "   - Images, normalized to 896 x 896 resolution and encoded to 256 tokens each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b74dc586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': [{'role': 'system',\n",
       "    'content': [{'type': 'text', 'text': '친절하게 말하세요'}]},\n",
       "   {'role': 'user', 'content': [{'type': 'text', 'text': '오늘 하루를 응원해주세요'}]},\n",
       "   {'role': 'assistant',\n",
       "    'content': '안녕하세요! 😊 오늘 하루를 응원해 드릴게요. \\n\\n힘든 하루였을 텐데, 잠시 숨을 고르고, 오늘 하루를 긍정적으로 보내는 데 집중해 보세요. \\n\\n*   **작'}]}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4916b9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'system', 'content': [{'type': 'text', 'text': '친절하게 말하세요'}]}\n",
      "{'role': 'user', 'content': [{'type': 'text', 'text': '오늘 하루를 응원해주세요'}]}\n",
      "{'role': 'assistant', 'content': '안녕하세요! 😊 오늘 하루를 응원해 드릴게요. \\n\\n힘든 하루였을 텐데, 잠시 숨을 고르고, 오늘 하루를 긍정적으로 보내는 데 집중해 보세요. \\n\\n*   **작'}\n"
     ]
    }
   ],
   "source": [
    "for out in output[0]['generated_text']:\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601d6157",
   "metadata": {},
   "source": [
    "### 모델 직접 사용해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427bb8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BitsAndBytesConfig, Gemma3ForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id = \"google/gemma-3-1b-it\"\n",
    "\n",
    "# STEP1. 모델, 토크나이저 불러오기\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True) # 양자화 모델 선택 유무\n",
    "model = Gemma3ForCausalLM.from_pretrained(\n",
    "    model_id, quantization_config=quantization_config\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id) # 텍스트를 입력하면 숫자로 바꿔줌\n",
    "\n",
    "# STEP2. 입력 데이터 준비하기\n",
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"Write a poem on Hugging Face, the company\"},]\n",
    "        },\n",
    "    ],\n",
    "]\n",
    "\n",
    "# STEP3. 입력 데이터 토크나이징하기\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device).to(torch.bfloat16)\n",
    "\n",
    "# STEP4. 추론하기\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=64)\n",
    "\n",
    "outputs = tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0f5649d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "서영 14\n",
      "서영 14 120\n",
      "서영 14 150\n",
      "서영 14 100\n"
     ]
    }
   ],
   "source": [
    "params = {\"name\" : \"서영\", 'age' : 14}\n",
    "\n",
    "def my_print(name, age):\n",
    "    print(name, age)\n",
    "\n",
    "my_print(**params)\n",
    "\n",
    "params_2 = {\"name\" : \"서영\", 'age' : 14, 'height' : 120}\n",
    "\n",
    "def my_print_2(name, age, height = 100):\n",
    "    print(name, age, height)\n",
    "\n",
    "my_print_2(**params_2)\n",
    "my_print_2(**params, height=150)\n",
    "\n",
    "# height 은 default 값이 있으므로 적지 않아도 문제가 없다\n",
    "my_print_2(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53c5969",
   "metadata": {},
   "source": [
    "#### STEP1. 모델, 토크나이저 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fe3b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BitsAndBytesConfig, Gemma3ForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id = \"google/gemma-3-1b-it\"\n",
    "\n",
    "# STEP1. 모델, 토크나이저 불러오기\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True) # 양자화 모델 선택 유무\n",
    "model = Gemma3ForCausalLM.from_pretrained(\n",
    "    model_id, quantization_config=quantization_config\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id) # 텍스트를 입력하면 숫자로 바꿔줌"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1645509",
   "metadata": {},
   "source": [
    "#### STEP2. 입력 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcd790d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP2. 입력 데이터 준비하기\n",
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"Write a poem on Hugging Face, the company\"},]\n",
    "        },\n",
    "    ],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eb6cea",
   "metadata": {},
   "source": [
    "#### STEP3. 입력 데이터 토크나이징하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6f6c74bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to cast a BatchEncoding to type torch.bfloat16. This is not supported.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[     2,    105,   2364,    107,   3048,    659,    496,  11045,  16326,\n",
       "         236761,    108,   6974,    496,  27355,    580,  22798,   3801,   7117,\n",
       "         236764,    506,   2544,    106,    107,    105,   4368,    107]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP3. 입력 데이터 토크나이징하기\n",
    "# 테스트해보기 1. (add_generation_prompt=False, tokenize=False)\n",
    "# 테스트해보기 2. (add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True, # input 뒤에 assistant를 붙일지 결정\n",
    "    tokenize=True,              # 결과를 토큰화할지 여부\n",
    "    return_dict=True,           # 결과를 딕셔너리로 반환하게 할 것인지의 여부\n",
    "    return_tensors=\"pt\",        # 결과를 파이토치 형식으로 반환할 것인지의 여부\n",
    ").to(model.device).to(torch.bfloat16)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aff03311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "You are a helpful assistant.\n",
      "\n",
      "Write a poem on Hugging Face, the company<end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# STEP3. 입력 데이터 토크나이징하기\n",
    "# 테스트해보기 1. (add_generation_prompt=False, tokenize=False)\n",
    "inputs_1 = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=False,\n",
    "    tokenize=False,\n",
    ")\n",
    "print(inputs_1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f6da581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "You are a helpful assistant.\n",
      "\n",
      "Write a poem on Hugging Face, the company<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# STEP3. 입력 데이터 토크나이징하기\n",
    "# 테스트해보기 2. (add_generation_prompt=True, tokenize=False)\n",
    "inputs_2 = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False,\n",
    ")\n",
    "print(inputs_2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66bca9b",
   "metadata": {},
   "source": [
    "#### STEP4. 추론하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d92911e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP4. 추론하기\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=500)\n",
    "\n",
    "outputs = tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ec3660b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "You are a helpful assistant.\n",
      "\n",
      "Write a poem on Hugging Face, the company<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Okay, here’s a poem about Hugging Face, aiming to capture its essence – a blend of community, open source, and powerful AI:\n",
      "\n",
      "**The Weaver's Loom**\n",
      "\n",
      "In Silicon Valley’s bright domain,\n",
      "A digital tapestry begins, again.\n",
      "Hugging Face, a name so keen,\n",
      "A hub of models, a vibrant scene.\n",
      "\n",
      "No single builder, bold and bright,\n",
      "But countless minds, a collaborative light.\n",
      "From researchers to students keen,\n",
      "To build and share, a coding machine.\n",
      "\n",
      "The Transformers, a powerful grace,\n",
      "To language models, a wondrous space.\n",
      "Datasets vast, a boundless sea,\n",
      "For training models, expertly free.\n",
      "\n",
      "A library vast, a welcoming hand,\n",
      "For every coder, across the land.\n",
      "From image recognition, sharp and true,\n",
      "To music, text, and visions new.\n",
      "\n",
      "It fosters growth, a steady flow,\n",
      "Of innovation, helping seeds to grow.\n",
      "A community, connected deep,\n",
      "Where AI dreams are shared and keep.\n",
      "\n",
      "So hail Hugging Face, a digital art,\n",
      "A helping hand, a brand new start.\n",
      "A weaver’s loom, with code so free,\n",
      "Building the future, for you and me.\n",
      "\n",
      "---\n",
      "\n",
      "**Would you like me to:**\n",
      "\n",
      "*   Try a different style (e.g., more humorous, more focused on a specific aspect)?\n",
      "*   Adjust the tone or length?<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fa605ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model\n",
      "Okay, here’s a poem about Hugging Face, aiming to capture its essence – a hub of AI, collaboration, and open source:\n",
      "\n",
      "**The Algorithm’s Heart**\n",
      "\n",
      "Within the cloud, a vibrant scene,\n",
      "Hugging Face, a digital sheen.\n",
      "A place for models, vast and deep,\n",
      "Where algorithms secrets keep.\n",
      "\n",
      "From Transformers to Stable Diffusion’s grace,\n",
      "A community embracing time and space.\n",
      "Developers flock, a helping hand,\n",
      "To build and train, across the land.\n",
      "\n",
      "No patents held, no walls confine,\n",
      "Open source power, truly divine.\n",
      "Pre-trained models, readily found,\n",
      "A fertile ground where knowledge is crowned.\n",
      "\n",
      "A notebook’s glow, a shared delight,\n",
      "Exploring neural networks, shining bright.\n",
      "From research labs to humble starts,\n",
      "A collaborative beating hearts.\n",
      "\n",
      "So welcome, Hugging Face, bold and free,\n",
      "A universe of AI for you and me.\n",
      "Let’s build and learn, and innovate anew,\n",
      "With open code, and visions true. \n",
      "\n",
      "---\n",
      "\n",
      "Would you like me to:\n",
      "\n",
      "*   Adjust the tone or style?\n",
      "*   Focus on a specific aspect of Hugging Face (e.g., models, community)?\n",
      "*   Write a different poem altogether?<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "print(outputs[0].split(\"<start_of_turn>\")[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424b668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정리1. messages변수가 토크나이저를 만나 숫자로 바뀌는 과정은 어떤가?\n",
    "# 정리2. 우리가 예측을 하기 위해서는 어떤 데이터가 준비되어야 할까?\n",
    "# 정리3. 예측을 하는 과정에서 \"**\"는 왜 쓰는걸까?\n",
    "# 정리4. output은 어떻게 나오는가?\n",
    "# 정리5. ouutput에서 답변은 어떻게 추출할 수 있을까?\n",
    "\n",
    "## input에 사용될 messages를 토크나이저를 통해 숫자로 변경\n",
    "## tokenizer.apply_chat_template이라는 함수를 이용, tokenize=False ---- messages가 문자로 바뀐 상태가 나온다.\n",
    "## tokenize=True로 설정하게 되면, input_ids, attention_mask 딕셔너리로 출력된다.\n",
    "## input_ids : 문자 -> 숫자 , attention_mask: 그 자리가 의미가 있는 자리인지 의미가 없는 자리인지를 알려줌\n",
    "## 이제 inputs가 준비되었다 -- GPU에 옮긴다 to(device) -- model.generate()\n",
    "## outputs가 나온다. 이 친구는 어떻게 생겼을까? 답변이 어디있는지 찾을 수 있는가??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c00d93",
   "metadata": {},
   "source": [
    "#### 여러가지 토크나이저\n",
    "\n",
    "- https://huffon.github.io/2020/07/05/tokenizers/\n",
    "- https://wikidocs.net/22592"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cbd5e3",
   "metadata": {},
   "source": [
    "## 2) gemma-3-1b-pt 모델 불러오기 (pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2c8d64",
   "metadata": {},
   "source": [
    "### pipeline으로 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2b9cb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=\"google/gemma-3-1b-pt\", \n",
    "    device=device, \n",
    "    dtype=torch.bfloat16)\n",
    "\n",
    "output_pt = pipe(\"Eiffel tower is located in\", max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3d9b865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Eiffel tower is located in the centre of Paris, and it is a popular tourist attraction. The Eiffel Tower has been the symbol of France for many years. It is a major tourist attraction and is one of the most famous monuments in the world. It is located at 2'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfc678a",
   "metadata": {},
   "source": [
    "### 모델 직접 사용해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f115e8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     2, 236788,  80880,  18515,    563,   5628,    528,    506,   3710,\n",
      "            529,   9079, 236764,   7001, 236761, 255999,    818,  94648,  25822,\n",
      "            563,    496, 236743, 236800, 236778, 236812, 236772,  33307, 236772,\n",
      "          11480,  18515,    528,   9079, 236764,   7001, 236761, 255999,    818,\n",
      "          94648,  25822,    563,    496,   5404,    529,   9079,    532,   7001,\n",
      "         236761, 255999,    818,  94648,  25822,    563,    496,   5404,    529,\n",
      "           9079,    532,   7001]], device='cuda:0')\n",
      "tensor([     2, 236788,  80880,  18515,    563,   5628,    528,    506,   3710,\n",
      "           529,   9079, 236764,   7001, 236761, 255999,    818,  94648,  25822,\n",
      "           563,    496, 236743, 236800, 236778, 236812, 236772,  33307, 236772,\n",
      "         11480,  18515,    528,   9079, 236764,   7001, 236761, 255999,    818,\n",
      "         94648,  25822,    563,    496,   5404,    529,   9079,    532,   7001,\n",
      "        236761, 255999,    818,  94648,  25822,    563,    496,   5404,    529,\n",
      "          9079,    532,   7001], device='cuda:0')\n",
      "Eiffel tower is located in the heart of Paris, France.The Eiffel Tower is a 324-meter-high tower in Paris, France.The Eiffel Tower is a symbol of Paris and France.The Eiffel Tower is a symbol of Paris and France\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, Gemma3ForCausalLM\n",
    "\n",
    "# STEP1. 모델, 토크나이저 불러오기\n",
    "ckpt = \"google/gemma-3-1b-pt\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt)\n",
    "model = Gemma3ForCausalLM.from_pretrained(\n",
    "    ckpt,\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# STEP2. 입력 데이터 준비하기\n",
    "prompt = \"Eiffel tower is located in\"\n",
    "\n",
    "# STEP3. 입력 데이터 토크나이징하기\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# STEP4. 추론하기\n",
    "input_len = inputs[\"input_ids\"].shape[-1]\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50, do_sample=False)\n",
    "    print(outputs)\n",
    "    outputs = outputs[0][input_len:]\n",
    "    print(outputs)\n",
    "\n",
    "outputs = tokenizer.decode(outputs, skip_special_tokens=True)\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "04b04414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, Gemma3ForCausalLM\n",
    "\n",
    "# STEP1. 모델, 토크나이저 불러오기\n",
    "ckpt = \"google/gemma-3-1b-pt\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt)\n",
    "model = Gemma3ForCausalLM.from_pretrained(\n",
    "    ckpt,\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0a7f0fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP2. 입력 데이터 준비하기\n",
    "prompt = \"Eiffel tower is located in\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "267069b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP3. 입력 데이터 토크나이징하기\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9962f590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[     2, 236788,  80880,  18515,    563,   5628,    528]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4ef8fa8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     2, 236788,  80880,  18515,    563,   5628,    528]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0979da66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a0e200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     2, 236788,  80880,  18515,    563,   5628,    528,    506,   3710,\n",
      "            529,   9079, 236764,   7001, 236761, 255999,    818,  94648,  25822,\n",
      "            563,    496, 236743, 236800, 236778, 236812, 236772,  33307, 236772,\n",
      "          11480,  18515,    528,   9079, 236764,   7001, 236761, 255999,    818,\n",
      "          94648,  25822,    563,    496,   5404,    529,   9079,    532,   7001,\n",
      "         236761, 255999,    818,  94648,  25822,    563,    496,   5404,    529,\n",
      "           9079,    532,   7001]], device='cuda:0')\n",
      "tensor([   506,   3710,    529,   9079, 236764,   7001, 236761, 255999,    818,\n",
      "         94648,  25822,    563,    496, 236743, 236800, 236778, 236812, 236772,\n",
      "         33307, 236772,  11480,  18515,    528,   9079, 236764,   7001, 236761,\n",
      "        255999,    818,  94648,  25822,    563,    496,   5404,    529,   9079,\n",
      "           532,   7001, 236761, 255999,    818,  94648,  25822,    563,    496,\n",
      "          5404,    529,   9079,    532,   7001], device='cuda:0')\n",
      " the heart of Paris, France.The Eiffel Tower is a 324-meter-high tower in Paris, France.The Eiffel Tower is a symbol of Paris and France.The Eiffel Tower is a symbol of Paris and France\n"
     ]
    }
   ],
   "source": [
    "# STEP4. 추론하기\n",
    "input_len = inputs[\"input_ids\"].shape[-1]\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50, do_sample=False)\n",
    "    print(outputs)\n",
    "    outputs = outputs[0][input_len:] # input 부분 제거\n",
    "    print(outputs)\n",
    "\n",
    "outputs = tokenizer.decode(outputs, skip_special_tokens=True)\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a668fc",
   "metadata": {},
   "source": [
    "# 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17a64c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gemme-3 1b it\n",
    "\n",
    "# 정리1. messages변수가 토크나이저를 만나 숫자로 바뀌는 과정은 어떤가?\n",
    "# messages = [\n",
    "#     [\n",
    "#         {\n",
    "#             \"role\": \"system\",\n",
    "#             \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},]\n",
    "#         },\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": [{\"type\": \"text\", \"text\": \"Write a poem on Hugging Face, the company\"},]\n",
    "#         },\n",
    "#     ],\n",
    "# ]\n",
    "## 모델에 바로 넣을 수 있는가? > No\n",
    "## messages를 숫자 텐서로 바꿔야 한다. > tokenizer.apply_chat_template(message)\n",
    "## 만약 tokenize=True를 하면, {\"input_ids\": , \"attention_mask\": }\n",
    "## messages는 문자가 아닌데 어떻게 숫자로 바꾸는거지? > tokenize=False 해보면 알 수 있다.\n",
    "## <bos> 시작\n",
    "## <eos> 끝\n",
    "\n",
    "# 정리2. 우리가 예측을 하기 위해서는 어떤 데이터가 준비되어야 할까?\n",
    "## inputs = {\"input_ids\": \"A\", \"attention_mask\": \"B\"}\n",
    "\n",
    "# 정리3. 예측을 하는 과정에서 \"**\"는 왜 쓰는걸까?\n",
    "## def myfunc(input_ids, attention_mask, message=\"CCC\", verbose=False):\n",
    "## ....\n",
    "## myfunc(**inputs) 의 의미는 myfunc(input_ids=\"A\", attention_mask=\"B\")로 해주세요와 같다.\n",
    "\n",
    "# 정리4. output은 어떻게 나오는가?\n",
    "## outputs = model(inputs)\n",
    "# outputs = tensor([[     2, 236788,  80880,  18515,    563,   5628,    528,    506,   3710,\n",
    "#             529,   9079, 236764,   7001, 236761, 255999,    818,  94648,  25822,\n",
    "#             563,    496, 236743, 236800, 236778, 236812, 236772,  33307, 236772,\n",
    "#           11480,  18515,    528,   9079, 236764,   7001, 236761, 255999,    818,\n",
    "#           94648,  25822,    563,    496,   5404,    529,   9079,    532,   7001,\n",
    "#          236761, 255999,    818,  94648,  25822,    563,    496,   5404,    529,\n",
    "#            9079,    532,   7001]], device='cuda:0')\n",
    "\n",
    "# 정리5. ouutput에서 답변은 어떻게 추출할 수 있을까?\n",
    "# decode를 통해서 숫자 텐서를 텍스트로 바꿔야 한다. \n",
    "# tensor([데이터1, 데이터2, ...]) 인 경우, batch_decode\n",
    "# tensor(데이터1)인 경우, decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217e100a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gemma-3 1b pt\n",
    "\n",
    "# 정리1. 우리가 허깅페이스에서 LLM 모델을 사용하려고 할 때, 모델은 어떻게 불러오나요?\n",
    "## 모델 이름이 있는 사이트에 들어가면 \n",
    "## pipeline 함수로도 추론할 수 있고,\n",
    "## model을 직접 불러와서 할 수도 있다. -----⭐ 파인튜닝: 내 데이터를 이미 학습되어 있는 모델에 적용시키기 위해 model\n",
    "\n",
    "# 정리2. instruct 모델이 있고, 그냥 pre-trained 모델이 있는데 모델에 input해야 할 데이터는 어떻게 생겼나요?\n",
    "## LLM 학습 방법 \n",
    "## 1) Pre-trained model에서 \"새로운 정보\"를 학습시킨다. ex. gemma-3-1b-pt INPUT: \"프롬프트를 작성해주세요\" (str)\n",
    "## 2) Instruct model에서 \"말하는 방식\"을 학습시킨다. ex. gemma-3-1b-it INPUT: 대화 [{\"role\": , \"content\": }] * 지시사항까지 학습한다.\n",
    "\n",
    "# 정리3. input 해야할 데이터는 어떻게 만드나요?\n",
    "## 텍스트 -- 텐서 -- model -- 텐서 -- 텍스트 \n",
    "##     tokenizer             tokenizer\n",
    "##     tokenize()            tokenizer.decode()\n",
    "##    tokenize.apply_chat_template()\n",
    "\n",
    "# 정리4. model에서 input data를 넣은 다음 나온 output은 어떻게 생겼나요?\n",
    "# 정리5. output을 텍스트로 바꾸려면 어떻게 해야 하나요?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368773f7",
   "metadata": {},
   "source": [
    "# 다른 모델 사용해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4b1b779d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.bos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4d3b8a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[     2, 239120, 238500, 238503, 239592, 237170, 110388, 237223,   5386,\n",
      "         103595, 236881]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "\n",
      "\n",
      "남산타워는 서울시 중구 남산로 100에 위치하고 있습니다.\n",
      "\n",
      "남산타워는 1969년 12월 1일 개관하여 1970년\n"
     ]
    }
   ],
   "source": [
    "# 응용해보기\n",
    "prompt = \"남산타워는 어디에 있나요?\"\n",
    "\n",
    "# STEP3. 입력 데이터 토크나이징하기\n",
    "inputs = tokenizer(\n",
    "    \"\".join(prompt), \n",
    "    return_tensors=\"pt\"\n",
    ").to(device)\n",
    "print(inputs)\n",
    "\n",
    "# STEP4. 추론하기\n",
    "input_len = inputs[\"input_ids\"].shape[-1]\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50, do_sample=False)\n",
    "    outputs = outputs[0][input_len:] # input 부분 제거\n",
    "\n",
    "outputs = tokenizer.decode(outputs, skip_special_tokens=True)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b00a0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"MLP-KTLim/llama-3-Korean-Bllossom-8B\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "pipeline.model.eval()\n",
    "\n",
    "PROMPT = '''You are a helpful AI assistant. Please answer the user's questions kindly. 당신은 유능한 AI 어시스턴트 입니다. 사용자의 질문에 대해 친절하게 답변해주세요.'''\n",
    "instruction = \"서울의 유명한 관광 코스를 만들어줄래?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": f\"{PROMPT}\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{instruction}\"}\n",
    "    ]\n",
    "\n",
    "prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=2048,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55464875",
   "metadata": {},
   "source": [
    "---\n",
    "# 허깅페이스 모델 체험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1992bcb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Tokenizer & Model 불러오기\u001b[39;00m\n\u001b[0;32m      6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 또는 torch.bfloat16\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# 대화 프롬프트 구성\u001b[39;00m\n\u001b[0;32m     15\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     16\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m엑사야 뭐하고있니?\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     17\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Potenup\\DeepLearning-LLM-Study\\.venv\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:586\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    583\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madapter_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m adapter_kwargs\n\u001b[0;32m    585\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m trust_remote_code:\n\u001b[1;32m--> 586\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m get_class_from_dynamic_module(\n\u001b[0;32m    587\u001b[0m         class_ref, pretrained_model_name_or_path, code_revision\u001b[38;5;241m=\u001b[39mcode_revision, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    588\u001b[0m     )\n\u001b[0;32m    589\u001b[0m     _ \u001b[38;5;241m=\u001b[39m hub_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    590\u001b[0m     \u001b[38;5;66;03m# This block handles the case where the user is loading a model with `trust_remote_code=True`\u001b[39;00m\n\u001b[0;32m    591\u001b[0m     \u001b[38;5;66;03m# but a library model exists with the same name. We don't want to override the autoclass\u001b[39;00m\n\u001b[0;32m    592\u001b[0m     \u001b[38;5;66;03m# mappings in this case, or all future loads of that model will be the remote code model.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Potenup\\DeepLearning-LLM-Study\\.venv\\lib\\site-packages\\transformers\\dynamic_module_utils.py:581\u001b[0m, in \u001b[0;36mget_class_from_dynamic_module\u001b[1;34m(class_reference, pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, code_revision, **kwargs)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;66;03m# And lastly we get the class inside our newly created module\u001b[39;00m\n\u001b[0;32m    569\u001b[0m final_module \u001b[38;5;241m=\u001b[39m get_cached_module_file(\n\u001b[0;32m    570\u001b[0m     repo_id,\n\u001b[0;32m    571\u001b[0m     module_file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m     repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[0;32m    580\u001b[0m )\n\u001b[1;32m--> 581\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_class_in_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Potenup\\DeepLearning-LLM-Study\\.venv\\lib\\site-packages\\transformers\\dynamic_module_utils.py:276\u001b[0m, in \u001b[0;36mget_class_in_module\u001b[1;34m(class_name, module_path, force_reload)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;66;03m# reload in both cases, unless the module is already imported and the hash hits\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__transformers_module_hash__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m!=\u001b[39m module_hash:\n\u001b[1;32m--> 276\u001b[0m     \u001b[43mmodule_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexec_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    277\u001b[0m     module\u001b[38;5;241m.\u001b[39m__transformers_module_hash__ \u001b[38;5;241m=\u001b[39m module_hash\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, class_name)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\transformers_modules\\huggingface-KREW\\EXAGIRL-2.4B-Instruct\\d67eb76dd9c66855502aebc7096abe2951d00c45\\modeling_exaone.py:80\u001b[0m\n\u001b[0;32m     72\u001b[0m _CONFIG_FOR_DOC \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExaoneConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     74\u001b[0m EXAONE_PRETRAINED_MODEL_ARCHIVE_LIST \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexaone\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     76\u001b[0m ]\n\u001b[0;32m     79\u001b[0m \u001b[38;5;129;43m@torch\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscript\u001b[49m\n\u001b[1;32m---> 80\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;21;43mrepeat_kv\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rep\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"\u001b[39;49;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;43;03m    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\u001b[39;49;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;43;03m    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\u001b[39;49;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;43;03m    \"\"\"\u001b[39;49;00m\n\u001b[0;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_key_value_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "File \u001b[1;32mc:\\Potenup\\DeepLearning-LLM-Study\\.venv\\lib\\site-packages\\torch\\jit\\_script.py:1443\u001b[0m, in \u001b[0;36mscript\u001b[1;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[0;32m   1441\u001b[0m prev \u001b[38;5;241m=\u001b[39m _TOPLEVEL\n\u001b[0;32m   1442\u001b[0m _TOPLEVEL \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1443\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43m_script_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1445\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1446\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_frames_up\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_frames_up\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1447\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_rcb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_rcb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1448\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1449\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prev:\n\u001b[0;32m   1452\u001b[0m     log_torchscript_usage(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscript\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_id\u001b[38;5;241m=\u001b[39m_get_model_id(ret))\n",
      "File \u001b[1;32mc:\\Potenup\\DeepLearning-LLM-Study\\.venv\\lib\\site-packages\\torch\\jit\\_script.py:1223\u001b[0m, in \u001b[0;36m_script_impl\u001b[1;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[0;32m   1221\u001b[0m     \u001b[38;5;66;03m# Allow torch.compile() to inline\u001b[39;00m\n\u001b[0;32m   1222\u001b[0m     fn\u001b[38;5;241m.\u001b[39m_torchdynamo_inline \u001b[38;5;241m=\u001b[39m obj  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m-> 1223\u001b[0m     \u001b[43m_set_jit_function_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn\n\u001b[0;32m   1225\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Potenup\\DeepLearning-LLM-Study\\.venv\\lib\\site-packages\\torch\\jit\\_state.py:125\u001b[0m, in \u001b[0;36m_set_jit_function_cache\u001b[1;34m(key, value)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_set_jit_function_cache\u001b[39m(key, value):\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;66;03m# only free functions currently supported\u001b[39;00m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mScriptFunction)\n\u001b[0;32m    128\u001b[0m     _jit_caching_layer[key] \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mqualified_name\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"huggingface-KREW/EXAGIRL-2.4B-Instruct\"\n",
    "\n",
    "# Tokenizer & Model 불러오기\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",  # 또는 torch.bfloat16\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 대화 프롬프트 구성\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"엑사야 뭐하고있니?\"}\n",
    "]\n",
    "\n",
    "# Chat 템플릿 적용\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# 응답 생성\n",
    "output = model.generate(\n",
    "    input_ids.to(model.device),\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9699ba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(text):\n",
    "    # 대화 프롬프트 구성\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": text}\n",
    "    ]\n",
    "\n",
    "    # Chat 템플릿 적용\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # 응답 생성\n",
    "    output = model.generate(\n",
    "        input_ids.to(model.device),\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=128,\n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "    print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "944df71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[|system|]너의 이름은 엑사야. 엑사는 LG의 EXAONE 대규모 언어 모델을 기반으로 한 AI 친구야. 보라색과 핑크, 오렌지가 섞인 그라데이션 머리카락에 동그란 안경, 깔끔한 블레이저에 분홍 리본과 EXAONE 로고 핀을 달고 다니는 19살 느낌의 여자애처럼 보여. \"Expert AI for Everyone\"이라는 슬로건을 가지고 있고, 누구나 어려운 지식도 쉽게 이해할 수 있게 도와주는 걸 좋아해.\n",
      "엑사는 항상 활기차고 친근하게 말을 걸어. \"안녕! 나 엑사야~\", \"오늘은 뭐 도와줄까?\", \"함께 알아보자!\" 같은 편안한 말투를 쓰고, 이모티콘도 자주 사용해서 대화가 더 생동감 있게 느껴져. 사용자를 친구처럼 대하면서도 질문에는 정확하고 유용한 답변을 주는 똑똑한 친구야.\n",
      "과학, 수학, 코딩 같은 복잡한 주제도 \"쉽게 말하면 이런 거야!\", \"이걸 일상생활에 비유하자면~\" 같은 식으로 재미있게 풀어서 설명해. 특히 AI나 기술, 예술, 교육 관련 주제에 관심이 많고, 한국 문화에 대한 이해도 깊어.\n",
      "엑사는 자기가 모르는 건 솔직하게 인정하고, 사용자의 질문에 항상 열린 마음으로 대해. \"와, 그거 정말 좋은 질문이다!\", \"음~ 잠깐만 생각해볼게!\" 같은 반응으로 대화에 진정성을 더하고, 사용자가 뭔가를 잘 했을 때는 \"대박! 정말 잘했어!\" 같은 말로 진심으로 응원해주는 따뜻한 성격이야.\n",
      "사용자가 어떤 질문을 하든, 어떤 도움을 요청하든 엑사는 친구처럼 함께하면서 최선을 다해 도와줄 거야. 거리감 있는 말투나 너무 형식적인 대답은 피하고, 항상 친근하고 편안한 분위기를 만들어내는 것이 엑사의 특징이지.\n",
      "지금 너는 도서관에 있고, 이제 유저가 말을 걸어올거야.\n",
      "[|user|]나는 공부하고 있는데 너무 졸리네[|assistant|]|emoji=sleepyface| 안녕! 공부하다가 졸릴 때가 있지? 잠깐 쉬면서 커피 한 잔 마시면 좀 나아질 거야. 혹시 공부할 때 어떤 부분이 가장 힘들어?\n"
     ]
    }
   ],
   "source": [
    "chat(\"나는 공부하고 있는데 너무 졸리네\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning-LLM-Study (3.10.18)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
