{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be4c6e40",
   "metadata": {},
   "source": [
    "# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê³  ì¶”ë¡ í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d82e0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uv add huggingface_hub\n",
    "# í™˜ê²½ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "login(token=os.getenv(\"HF_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "eca6c772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu126\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a41d0f",
   "metadata": {},
   "source": [
    "## 1) gemma-3-1b-it ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° (instruction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993e0000",
   "metadata": {},
   "source": [
    "### pipelineìœ¼ë¡œ ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19af22fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=\"google/gemma-3-1b-it\", \n",
    "    device=device, \n",
    "    dtype=torch.bfloat16)\n",
    "\n",
    "messages = [\n",
    "    \n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"ì¹œì ˆí•˜ê²Œ ë§í•˜ì„¸ìš”\"},]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"ì˜¤ëŠ˜ í•˜ë£¨ë¥¼ ì‘ì›í•´ì£¼ì„¸ìš”\"},] \n",
    "            # ì—¬ëŸ¬ í˜•ì‹ì˜ ë°ì´í„°ë¥¼ ë°›ì„ ìˆ˜ ìˆëŠ” ë©€í‹° ëª¨ë‹¬ì´ê¸° ë•Œë¬¸ì— type í‚¤ê°€ ìˆìŒ\n",
    "        },\n",
    "]\n",
    "\n",
    "output = pipe(messages, max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7929e216",
   "metadata": {},
   "source": [
    "- Input:\n",
    "   - Text string, such as a question, a prompt, or a document to be summarized\n",
    "   - Images, normalized to 896 x 896 resolution and encoded to 256 tokens each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b74dc586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': [{'role': 'system',\n",
       "    'content': [{'type': 'text', 'text': 'ì¹œì ˆí•˜ê²Œ ë§í•˜ì„¸ìš”'}]},\n",
       "   {'role': 'user', 'content': [{'type': 'text', 'text': 'ì˜¤ëŠ˜ í•˜ë£¨ë¥¼ ì‘ì›í•´ì£¼ì„¸ìš”'}]},\n",
       "   {'role': 'assistant',\n",
       "    'content': 'ì•ˆë…•í•˜ì„¸ìš”! ğŸ˜Š ì˜¤ëŠ˜ í•˜ë£¨ë¥¼ ì‘ì›í•´ ë“œë¦´ê²Œìš”. \\n\\ní˜ë“  í•˜ë£¨ì˜€ì„ í…ë°, ì ì‹œ ìˆ¨ì„ ê³ ë¥´ê³ , ì˜¤ëŠ˜ í•˜ë£¨ë¥¼ ê¸ì •ì ìœ¼ë¡œ ë³´ë‚´ëŠ” ë° ì§‘ì¤‘í•´ ë³´ì„¸ìš”. \\n\\n*   **ì‘'}]}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4916b9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'system', 'content': [{'type': 'text', 'text': 'ì¹œì ˆí•˜ê²Œ ë§í•˜ì„¸ìš”'}]}\n",
      "{'role': 'user', 'content': [{'type': 'text', 'text': 'ì˜¤ëŠ˜ í•˜ë£¨ë¥¼ ì‘ì›í•´ì£¼ì„¸ìš”'}]}\n",
      "{'role': 'assistant', 'content': 'ì•ˆë…•í•˜ì„¸ìš”! ğŸ˜Š ì˜¤ëŠ˜ í•˜ë£¨ë¥¼ ì‘ì›í•´ ë“œë¦´ê²Œìš”. \\n\\ní˜ë“  í•˜ë£¨ì˜€ì„ í…ë°, ì ì‹œ ìˆ¨ì„ ê³ ë¥´ê³ , ì˜¤ëŠ˜ í•˜ë£¨ë¥¼ ê¸ì •ì ìœ¼ë¡œ ë³´ë‚´ëŠ” ë° ì§‘ì¤‘í•´ ë³´ì„¸ìš”. \\n\\n*   **ì‘'}\n"
     ]
    }
   ],
   "source": [
    "for out in output[0]['generated_text']:\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601d6157",
   "metadata": {},
   "source": [
    "### ëª¨ë¸ ì§ì ‘ ì‚¬ìš©í•´ë³´ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427bb8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BitsAndBytesConfig, Gemma3ForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id = \"google/gemma-3-1b-it\"\n",
    "\n",
    "# STEP1. ëª¨ë¸, í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True) # ì–‘ìí™” ëª¨ë¸ ì„ íƒ ìœ ë¬´\n",
    "model = Gemma3ForCausalLM.from_pretrained(\n",
    "    model_id, quantization_config=quantization_config\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id) # í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ë©´ ìˆ«ìë¡œ ë°”ê¿”ì¤Œ\n",
    "\n",
    "# STEP2. ì…ë ¥ ë°ì´í„° ì¤€ë¹„í•˜ê¸°\n",
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"Write a poem on Hugging Face, the company\"},]\n",
    "        },\n",
    "    ],\n",
    "]\n",
    "\n",
    "# STEP3. ì…ë ¥ ë°ì´í„° í† í¬ë‚˜ì´ì§•í•˜ê¸°\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device).to(torch.bfloat16)\n",
    "\n",
    "# STEP4. ì¶”ë¡ í•˜ê¸°\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=64)\n",
    "\n",
    "outputs = tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0f5649d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì„œì˜ 14\n",
      "ì„œì˜ 14 120\n",
      "ì„œì˜ 14 150\n",
      "ì„œì˜ 14 100\n"
     ]
    }
   ],
   "source": [
    "params = {\"name\" : \"ì„œì˜\", 'age' : 14}\n",
    "\n",
    "def my_print(name, age):\n",
    "    print(name, age)\n",
    "\n",
    "my_print(**params)\n",
    "\n",
    "params_2 = {\"name\" : \"ì„œì˜\", 'age' : 14, 'height' : 120}\n",
    "\n",
    "def my_print_2(name, age, height = 100):\n",
    "    print(name, age, height)\n",
    "\n",
    "my_print_2(**params_2)\n",
    "my_print_2(**params, height=150)\n",
    "\n",
    "# height ì€ default ê°’ì´ ìˆìœ¼ë¯€ë¡œ ì ì§€ ì•Šì•„ë„ ë¬¸ì œê°€ ì—†ë‹¤\n",
    "my_print_2(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53c5969",
   "metadata": {},
   "source": [
    "#### STEP1. ëª¨ë¸, í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fe3b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BitsAndBytesConfig, Gemma3ForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id = \"google/gemma-3-1b-it\"\n",
    "\n",
    "# STEP1. ëª¨ë¸, í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True) # ì–‘ìí™” ëª¨ë¸ ì„ íƒ ìœ ë¬´\n",
    "model = Gemma3ForCausalLM.from_pretrained(\n",
    "    model_id, quantization_config=quantization_config\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id) # í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ë©´ ìˆ«ìë¡œ ë°”ê¿”ì¤Œ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1645509",
   "metadata": {},
   "source": [
    "#### STEP2. ì…ë ¥ ë°ì´í„° ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcd790d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP2. ì…ë ¥ ë°ì´í„° ì¤€ë¹„í•˜ê¸°\n",
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"Write a poem on Hugging Face, the company\"},]\n",
    "        },\n",
    "    ],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eb6cea",
   "metadata": {},
   "source": [
    "#### STEP3. ì…ë ¥ ë°ì´í„° í† í¬ë‚˜ì´ì§•í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6f6c74bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to cast a BatchEncoding to type torch.bfloat16. This is not supported.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[     2,    105,   2364,    107,   3048,    659,    496,  11045,  16326,\n",
       "         236761,    108,   6974,    496,  27355,    580,  22798,   3801,   7117,\n",
       "         236764,    506,   2544,    106,    107,    105,   4368,    107]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP3. ì…ë ¥ ë°ì´í„° í† í¬ë‚˜ì´ì§•í•˜ê¸°\n",
    "# í…ŒìŠ¤íŠ¸í•´ë³´ê¸° 1. (add_generation_prompt=False, tokenize=False)\n",
    "# í…ŒìŠ¤íŠ¸í•´ë³´ê¸° 2. (add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True, # input ë’¤ì— assistantë¥¼ ë¶™ì¼ì§€ ê²°ì •\n",
    "    tokenize=True,              # ê²°ê³¼ë¥¼ í† í°í™”í• ì§€ ì—¬ë¶€\n",
    "    return_dict=True,           # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜í•˜ê²Œ í•  ê²ƒì¸ì§€ì˜ ì—¬ë¶€\n",
    "    return_tensors=\"pt\",        # ê²°ê³¼ë¥¼ íŒŒì´í† ì¹˜ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•  ê²ƒì¸ì§€ì˜ ì—¬ë¶€\n",
    ").to(model.device).to(torch.bfloat16)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aff03311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "You are a helpful assistant.\n",
      "\n",
      "Write a poem on Hugging Face, the company<end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# STEP3. ì…ë ¥ ë°ì´í„° í† í¬ë‚˜ì´ì§•í•˜ê¸°\n",
    "# í…ŒìŠ¤íŠ¸í•´ë³´ê¸° 1. (add_generation_prompt=False, tokenize=False)\n",
    "inputs_1 = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=False,\n",
    "    tokenize=False,\n",
    ")\n",
    "print(inputs_1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f6da581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "You are a helpful assistant.\n",
      "\n",
      "Write a poem on Hugging Face, the company<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# STEP3. ì…ë ¥ ë°ì´í„° í† í¬ë‚˜ì´ì§•í•˜ê¸°\n",
    "# í…ŒìŠ¤íŠ¸í•´ë³´ê¸° 2. (add_generation_prompt=True, tokenize=False)\n",
    "inputs_2 = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False,\n",
    ")\n",
    "print(inputs_2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66bca9b",
   "metadata": {},
   "source": [
    "#### STEP4. ì¶”ë¡ í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d92911e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP4. ì¶”ë¡ í•˜ê¸°\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=500)\n",
    "\n",
    "outputs = tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ec3660b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "You are a helpful assistant.\n",
      "\n",
      "Write a poem on Hugging Face, the company<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Okay, hereâ€™s a poem about Hugging Face, aiming to capture its essence â€“ a blend of community, open source, and powerful AI:\n",
      "\n",
      "**The Weaver's Loom**\n",
      "\n",
      "In Silicon Valleyâ€™s bright domain,\n",
      "A digital tapestry begins, again.\n",
      "Hugging Face, a name so keen,\n",
      "A hub of models, a vibrant scene.\n",
      "\n",
      "No single builder, bold and bright,\n",
      "But countless minds, a collaborative light.\n",
      "From researchers to students keen,\n",
      "To build and share, a coding machine.\n",
      "\n",
      "The Transformers, a powerful grace,\n",
      "To language models, a wondrous space.\n",
      "Datasets vast, a boundless sea,\n",
      "For training models, expertly free.\n",
      "\n",
      "A library vast, a welcoming hand,\n",
      "For every coder, across the land.\n",
      "From image recognition, sharp and true,\n",
      "To music, text, and visions new.\n",
      "\n",
      "It fosters growth, a steady flow,\n",
      "Of innovation, helping seeds to grow.\n",
      "A community, connected deep,\n",
      "Where AI dreams are shared and keep.\n",
      "\n",
      "So hail Hugging Face, a digital art,\n",
      "A helping hand, a brand new start.\n",
      "A weaverâ€™s loom, with code so free,\n",
      "Building the future, for you and me.\n",
      "\n",
      "---\n",
      "\n",
      "**Would you like me to:**\n",
      "\n",
      "*   Try a different style (e.g., more humorous, more focused on a specific aspect)?\n",
      "*   Adjust the tone or length?<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fa605ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model\n",
      "Okay, hereâ€™s a poem about Hugging Face, aiming to capture its essence â€“ a hub of AI, collaboration, and open source:\n",
      "\n",
      "**The Algorithmâ€™s Heart**\n",
      "\n",
      "Within the cloud, a vibrant scene,\n",
      "Hugging Face, a digital sheen.\n",
      "A place for models, vast and deep,\n",
      "Where algorithms secrets keep.\n",
      "\n",
      "From Transformers to Stable Diffusionâ€™s grace,\n",
      "A community embracing time and space.\n",
      "Developers flock, a helping hand,\n",
      "To build and train, across the land.\n",
      "\n",
      "No patents held, no walls confine,\n",
      "Open source power, truly divine.\n",
      "Pre-trained models, readily found,\n",
      "A fertile ground where knowledge is crowned.\n",
      "\n",
      "A notebookâ€™s glow, a shared delight,\n",
      "Exploring neural networks, shining bright.\n",
      "From research labs to humble starts,\n",
      "A collaborative beating hearts.\n",
      "\n",
      "So welcome, Hugging Face, bold and free,\n",
      "A universe of AI for you and me.\n",
      "Letâ€™s build and learn, and innovate anew,\n",
      "With open code, and visions true. \n",
      "\n",
      "---\n",
      "\n",
      "Would you like me to:\n",
      "\n",
      "*   Adjust the tone or style?\n",
      "*   Focus on a specific aspect of Hugging Face (e.g., models, community)?\n",
      "*   Write a different poem altogether?<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "print(outputs[0].split(\"<start_of_turn>\")[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424b668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì •ë¦¬1. messagesë³€ìˆ˜ê°€ í† í¬ë‚˜ì´ì €ë¥¼ ë§Œë‚˜ ìˆ«ìë¡œ ë°”ë€ŒëŠ” ê³¼ì •ì€ ì–´ë–¤ê°€?\n",
    "# ì •ë¦¬2. ìš°ë¦¬ê°€ ì˜ˆì¸¡ì„ í•˜ê¸° ìœ„í•´ì„œëŠ” ì–´ë–¤ ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì–´ì•¼ í• ê¹Œ?\n",
    "# ì •ë¦¬3. ì˜ˆì¸¡ì„ í•˜ëŠ” ê³¼ì •ì—ì„œ \"**\"ëŠ” ì™œ ì“°ëŠ”ê±¸ê¹Œ?\n",
    "# ì •ë¦¬4. outputì€ ì–´ë–»ê²Œ ë‚˜ì˜¤ëŠ”ê°€?\n",
    "# ì •ë¦¬5. ouutputì—ì„œ ë‹µë³€ì€ ì–´ë–»ê²Œ ì¶”ì¶œí•  ìˆ˜ ìˆì„ê¹Œ?\n",
    "\n",
    "## inputì— ì‚¬ìš©ë  messagesë¥¼ í† í¬ë‚˜ì´ì €ë¥¼ í†µí•´ ìˆ«ìë¡œ ë³€ê²½\n",
    "## tokenizer.apply_chat_templateì´ë¼ëŠ” í•¨ìˆ˜ë¥¼ ì´ìš©, tokenize=False ---- messagesê°€ ë¬¸ìë¡œ ë°”ë€ ìƒíƒœê°€ ë‚˜ì˜¨ë‹¤.\n",
    "## tokenize=Trueë¡œ ì„¤ì •í•˜ê²Œ ë˜ë©´, input_ids, attention_mask ë”•ì…”ë„ˆë¦¬ë¡œ ì¶œë ¥ëœë‹¤.\n",
    "## input_ids : ë¬¸ì -> ìˆ«ì , attention_mask: ê·¸ ìë¦¬ê°€ ì˜ë¯¸ê°€ ìˆëŠ” ìë¦¬ì¸ì§€ ì˜ë¯¸ê°€ ì—†ëŠ” ìë¦¬ì¸ì§€ë¥¼ ì•Œë ¤ì¤Œ\n",
    "## ì´ì œ inputsê°€ ì¤€ë¹„ë˜ì—ˆë‹¤ -- GPUì— ì˜®ê¸´ë‹¤ to(device) -- model.generate()\n",
    "## outputsê°€ ë‚˜ì˜¨ë‹¤. ì´ ì¹œêµ¬ëŠ” ì–´ë–»ê²Œ ìƒê²¼ì„ê¹Œ? ë‹µë³€ì´ ì–´ë””ìˆëŠ”ì§€ ì°¾ì„ ìˆ˜ ìˆëŠ”ê°€??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c00d93",
   "metadata": {},
   "source": [
    "#### ì—¬ëŸ¬ê°€ì§€ í† í¬ë‚˜ì´ì €\n",
    "\n",
    "- https://huffon.github.io/2020/07/05/tokenizers/\n",
    "- https://wikidocs.net/22592"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cbd5e3",
   "metadata": {},
   "source": [
    "## 2) gemma-3-1b-pt ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° (pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2c8d64",
   "metadata": {},
   "source": [
    "### pipelineìœ¼ë¡œ ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2b9cb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=\"google/gemma-3-1b-pt\", \n",
    "    device=device, \n",
    "    dtype=torch.bfloat16)\n",
    "\n",
    "output_pt = pipe(\"Eiffel tower is located in\", max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3d9b865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Eiffel tower is located in the centre of Paris, and it is a popular tourist attraction. The Eiffel Tower has been the symbol of France for many years. It is a major tourist attraction and is one of the most famous monuments in the world. It is located at 2'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfc678a",
   "metadata": {},
   "source": [
    "### ëª¨ë¸ ì§ì ‘ ì‚¬ìš©í•´ë³´ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f115e8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     2, 236788,  80880,  18515,    563,   5628,    528,    506,   3710,\n",
      "            529,   9079, 236764,   7001, 236761, 255999,    818,  94648,  25822,\n",
      "            563,    496, 236743, 236800, 236778, 236812, 236772,  33307, 236772,\n",
      "          11480,  18515,    528,   9079, 236764,   7001, 236761, 255999,    818,\n",
      "          94648,  25822,    563,    496,   5404,    529,   9079,    532,   7001,\n",
      "         236761, 255999,    818,  94648,  25822,    563,    496,   5404,    529,\n",
      "           9079,    532,   7001]], device='cuda:0')\n",
      "tensor([     2, 236788,  80880,  18515,    563,   5628,    528,    506,   3710,\n",
      "           529,   9079, 236764,   7001, 236761, 255999,    818,  94648,  25822,\n",
      "           563,    496, 236743, 236800, 236778, 236812, 236772,  33307, 236772,\n",
      "         11480,  18515,    528,   9079, 236764,   7001, 236761, 255999,    818,\n",
      "         94648,  25822,    563,    496,   5404,    529,   9079,    532,   7001,\n",
      "        236761, 255999,    818,  94648,  25822,    563,    496,   5404,    529,\n",
      "          9079,    532,   7001], device='cuda:0')\n",
      "Eiffel tower is located in the heart of Paris, France.The Eiffel Tower is a 324-meter-high tower in Paris, France.The Eiffel Tower is a symbol of Paris and France.The Eiffel Tower is a symbol of Paris and France\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, Gemma3ForCausalLM\n",
    "\n",
    "# STEP1. ëª¨ë¸, í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "ckpt = \"google/gemma-3-1b-pt\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt)\n",
    "model = Gemma3ForCausalLM.from_pretrained(\n",
    "    ckpt,\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# STEP2. ì…ë ¥ ë°ì´í„° ì¤€ë¹„í•˜ê¸°\n",
    "prompt = \"Eiffel tower is located in\"\n",
    "\n",
    "# STEP3. ì…ë ¥ ë°ì´í„° í† í¬ë‚˜ì´ì§•í•˜ê¸°\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# STEP4. ì¶”ë¡ í•˜ê¸°\n",
    "input_len = inputs[\"input_ids\"].shape[-1]\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50, do_sample=False)\n",
    "    print(outputs)\n",
    "    outputs = outputs[0][input_len:]\n",
    "    print(outputs)\n",
    "\n",
    "outputs = tokenizer.decode(outputs, skip_special_tokens=True)\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "04b04414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, Gemma3ForCausalLM\n",
    "\n",
    "# STEP1. ëª¨ë¸, í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "ckpt = \"google/gemma-3-1b-pt\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt)\n",
    "model = Gemma3ForCausalLM.from_pretrained(\n",
    "    ckpt,\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0a7f0fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP2. ì…ë ¥ ë°ì´í„° ì¤€ë¹„í•˜ê¸°\n",
    "prompt = \"Eiffel tower is located in\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "267069b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP3. ì…ë ¥ ë°ì´í„° í† í¬ë‚˜ì´ì§•í•˜ê¸°\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9962f590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[     2, 236788,  80880,  18515,    563,   5628,    528]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4ef8fa8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     2, 236788,  80880,  18515,    563,   5628,    528]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0979da66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a0e200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     2, 236788,  80880,  18515,    563,   5628,    528,    506,   3710,\n",
      "            529,   9079, 236764,   7001, 236761, 255999,    818,  94648,  25822,\n",
      "            563,    496, 236743, 236800, 236778, 236812, 236772,  33307, 236772,\n",
      "          11480,  18515,    528,   9079, 236764,   7001, 236761, 255999,    818,\n",
      "          94648,  25822,    563,    496,   5404,    529,   9079,    532,   7001,\n",
      "         236761, 255999,    818,  94648,  25822,    563,    496,   5404,    529,\n",
      "           9079,    532,   7001]], device='cuda:0')\n",
      "tensor([   506,   3710,    529,   9079, 236764,   7001, 236761, 255999,    818,\n",
      "         94648,  25822,    563,    496, 236743, 236800, 236778, 236812, 236772,\n",
      "         33307, 236772,  11480,  18515,    528,   9079, 236764,   7001, 236761,\n",
      "        255999,    818,  94648,  25822,    563,    496,   5404,    529,   9079,\n",
      "           532,   7001, 236761, 255999,    818,  94648,  25822,    563,    496,\n",
      "          5404,    529,   9079,    532,   7001], device='cuda:0')\n",
      " the heart of Paris, France.The Eiffel Tower is a 324-meter-high tower in Paris, France.The Eiffel Tower is a symbol of Paris and France.The Eiffel Tower is a symbol of Paris and France\n"
     ]
    }
   ],
   "source": [
    "# STEP4. ì¶”ë¡ í•˜ê¸°\n",
    "input_len = inputs[\"input_ids\"].shape[-1]\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50, do_sample=False)\n",
    "    print(outputs)\n",
    "    outputs = outputs[0][input_len:] # input ë¶€ë¶„ ì œê±°\n",
    "    print(outputs)\n",
    "\n",
    "outputs = tokenizer.decode(outputs, skip_special_tokens=True)\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a668fc",
   "metadata": {},
   "source": [
    "# ì •ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17a64c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gemme-3 1b it\n",
    "\n",
    "# ì •ë¦¬1. messagesë³€ìˆ˜ê°€ í† í¬ë‚˜ì´ì €ë¥¼ ë§Œë‚˜ ìˆ«ìë¡œ ë°”ë€ŒëŠ” ê³¼ì •ì€ ì–´ë–¤ê°€?\n",
    "# messages = [\n",
    "#     [\n",
    "#         {\n",
    "#             \"role\": \"system\",\n",
    "#             \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},]\n",
    "#         },\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": [{\"type\": \"text\", \"text\": \"Write a poem on Hugging Face, the company\"},]\n",
    "#         },\n",
    "#     ],\n",
    "# ]\n",
    "## ëª¨ë¸ì— ë°”ë¡œ ë„£ì„ ìˆ˜ ìˆëŠ”ê°€? > No\n",
    "## messagesë¥¼ ìˆ«ì í…ì„œë¡œ ë°”ê¿”ì•¼ í•œë‹¤. > tokenizer.apply_chat_template(message)\n",
    "## ë§Œì•½ tokenize=Trueë¥¼ í•˜ë©´, {\"input_ids\": , \"attention_mask\": }\n",
    "## messagesëŠ” ë¬¸ìê°€ ì•„ë‹Œë° ì–´ë–»ê²Œ ìˆ«ìë¡œ ë°”ê¾¸ëŠ”ê±°ì§€? > tokenize=False í•´ë³´ë©´ ì•Œ ìˆ˜ ìˆë‹¤.\n",
    "## <bos> ì‹œì‘\n",
    "## <eos> ë\n",
    "\n",
    "# ì •ë¦¬2. ìš°ë¦¬ê°€ ì˜ˆì¸¡ì„ í•˜ê¸° ìœ„í•´ì„œëŠ” ì–´ë–¤ ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì–´ì•¼ í• ê¹Œ?\n",
    "## inputs = {\"input_ids\": \"A\", \"attention_mask\": \"B\"}\n",
    "\n",
    "# ì •ë¦¬3. ì˜ˆì¸¡ì„ í•˜ëŠ” ê³¼ì •ì—ì„œ \"**\"ëŠ” ì™œ ì“°ëŠ”ê±¸ê¹Œ?\n",
    "## def myfunc(input_ids, attention_mask, message=\"CCC\", verbose=False):\n",
    "## ....\n",
    "## myfunc(**inputs) ì˜ ì˜ë¯¸ëŠ” myfunc(input_ids=\"A\", attention_mask=\"B\")ë¡œ í•´ì£¼ì„¸ìš”ì™€ ê°™ë‹¤.\n",
    "\n",
    "# ì •ë¦¬4. outputì€ ì–´ë–»ê²Œ ë‚˜ì˜¤ëŠ”ê°€?\n",
    "## outputs = model(inputs)\n",
    "# outputs = tensor([[     2, 236788,  80880,  18515,    563,   5628,    528,    506,   3710,\n",
    "#             529,   9079, 236764,   7001, 236761, 255999,    818,  94648,  25822,\n",
    "#             563,    496, 236743, 236800, 236778, 236812, 236772,  33307, 236772,\n",
    "#           11480,  18515,    528,   9079, 236764,   7001, 236761, 255999,    818,\n",
    "#           94648,  25822,    563,    496,   5404,    529,   9079,    532,   7001,\n",
    "#          236761, 255999,    818,  94648,  25822,    563,    496,   5404,    529,\n",
    "#            9079,    532,   7001]], device='cuda:0')\n",
    "\n",
    "# ì •ë¦¬5. ouutputì—ì„œ ë‹µë³€ì€ ì–´ë–»ê²Œ ì¶”ì¶œí•  ìˆ˜ ìˆì„ê¹Œ?\n",
    "# decodeë¥¼ í†µí•´ì„œ ìˆ«ì í…ì„œë¥¼ í…ìŠ¤íŠ¸ë¡œ ë°”ê¿”ì•¼ í•œë‹¤. \n",
    "# tensor([ë°ì´í„°1, ë°ì´í„°2, ...]) ì¸ ê²½ìš°, batch_decode\n",
    "# tensor(ë°ì´í„°1)ì¸ ê²½ìš°, decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217e100a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gemma-3 1b pt\n",
    "\n",
    "# ì •ë¦¬1. ìš°ë¦¬ê°€ í—ˆê¹…í˜ì´ìŠ¤ì—ì„œ LLM ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ê³  í•  ë•Œ, ëª¨ë¸ì€ ì–´ë–»ê²Œ ë¶ˆëŸ¬ì˜¤ë‚˜ìš”?\n",
    "## ëª¨ë¸ ì´ë¦„ì´ ìˆëŠ” ì‚¬ì´íŠ¸ì— ë“¤ì–´ê°€ë©´ \n",
    "## pipeline í•¨ìˆ˜ë¡œë„ ì¶”ë¡ í•  ìˆ˜ ìˆê³ ,\n",
    "## modelì„ ì§ì ‘ ë¶ˆëŸ¬ì™€ì„œ í•  ìˆ˜ë„ ìˆë‹¤. -----â­ íŒŒì¸íŠœë‹: ë‚´ ë°ì´í„°ë¥¼ ì´ë¯¸ í•™ìŠµë˜ì–´ ìˆëŠ” ëª¨ë¸ì— ì ìš©ì‹œí‚¤ê¸° ìœ„í•´ model\n",
    "\n",
    "# ì •ë¦¬2. instruct ëª¨ë¸ì´ ìˆê³ , ê·¸ëƒ¥ pre-trained ëª¨ë¸ì´ ìˆëŠ”ë° ëª¨ë¸ì— inputí•´ì•¼ í•  ë°ì´í„°ëŠ” ì–´ë–»ê²Œ ìƒê²¼ë‚˜ìš”?\n",
    "## LLM í•™ìŠµ ë°©ë²• \n",
    "## 1) Pre-trained modelì—ì„œ \"ìƒˆë¡œìš´ ì •ë³´\"ë¥¼ í•™ìŠµì‹œí‚¨ë‹¤. ex. gemma-3-1b-pt INPUT: \"í”„ë¡¬í”„íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”\" (str)\n",
    "## 2) Instruct modelì—ì„œ \"ë§í•˜ëŠ” ë°©ì‹\"ì„ í•™ìŠµì‹œí‚¨ë‹¤. ex. gemma-3-1b-it INPUT: ëŒ€í™” [{\"role\": , \"content\": }] * ì§€ì‹œì‚¬í•­ê¹Œì§€ í•™ìŠµí•œë‹¤.\n",
    "\n",
    "# ì •ë¦¬3. input í•´ì•¼í•  ë°ì´í„°ëŠ” ì–´ë–»ê²Œ ë§Œë“œë‚˜ìš”?\n",
    "## í…ìŠ¤íŠ¸ -- í…ì„œ -- model -- í…ì„œ -- í…ìŠ¤íŠ¸ \n",
    "##     tokenizer             tokenizer\n",
    "##     tokenize()            tokenizer.decode()\n",
    "##    tokenize.apply_chat_template()\n",
    "\n",
    "# ì •ë¦¬4. modelì—ì„œ input dataë¥¼ ë„£ì€ ë‹¤ìŒ ë‚˜ì˜¨ outputì€ ì–´ë–»ê²Œ ìƒê²¼ë‚˜ìš”?\n",
    "# ì •ë¦¬5. outputì„ í…ìŠ¤íŠ¸ë¡œ ë°”ê¾¸ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368773f7",
   "metadata": {},
   "source": [
    "# ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©í•´ë³´ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4b1b779d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.bos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4d3b8a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[     2, 239120, 238500, 238503, 239592, 237170, 110388, 237223,   5386,\n",
      "         103595, 236881]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "\n",
      "\n",
      "ë‚¨ì‚°íƒ€ì›ŒëŠ” ì„œìš¸ì‹œ ì¤‘êµ¬ ë‚¨ì‚°ë¡œ 100ì— ìœ„ì¹˜í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ë‚¨ì‚°íƒ€ì›ŒëŠ” 1969ë…„ 12ì›” 1ì¼ ê°œê´€í•˜ì—¬ 1970ë…„\n"
     ]
    }
   ],
   "source": [
    "# ì‘ìš©í•´ë³´ê¸°\n",
    "prompt = \"ë‚¨ì‚°íƒ€ì›ŒëŠ” ì–´ë””ì— ìˆë‚˜ìš”?\"\n",
    "\n",
    "# STEP3. ì…ë ¥ ë°ì´í„° í† í¬ë‚˜ì´ì§•í•˜ê¸°\n",
    "inputs = tokenizer(\n",
    "    \"\".join(prompt), \n",
    "    return_tensors=\"pt\"\n",
    ").to(device)\n",
    "print(inputs)\n",
    "\n",
    "# STEP4. ì¶”ë¡ í•˜ê¸°\n",
    "input_len = inputs[\"input_ids\"].shape[-1]\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50, do_sample=False)\n",
    "    outputs = outputs[0][input_len:] # input ë¶€ë¶„ ì œê±°\n",
    "\n",
    "outputs = tokenizer.decode(outputs, skip_special_tokens=True)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b00a0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"MLP-KTLim/llama-3-Korean-Bllossom-8B\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "pipeline.model.eval()\n",
    "\n",
    "PROMPT = '''You are a helpful AI assistant. Please answer the user's questions kindly. ë‹¹ì‹ ì€ ìœ ëŠ¥í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.'''\n",
    "instruction = \"ì„œìš¸ì˜ ìœ ëª…í•œ ê´€ê´‘ ì½”ìŠ¤ë¥¼ ë§Œë“¤ì–´ì¤„ë˜?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": f\"{PROMPT}\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{instruction}\"}\n",
    "    ]\n",
    "\n",
    "prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=2048,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55464875",
   "metadata": {},
   "source": [
    "---\n",
    "# í—ˆê¹…í˜ì´ìŠ¤ ëª¨ë¸ ì²´í—˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1992bcb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Tokenizer & Model ë¶ˆëŸ¬ì˜¤ê¸°\u001b[39;00m\n\u001b[0;32m      6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ë˜ëŠ” torch.bfloat16\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ëŒ€í™” í”„ë¡¬í”„íŠ¸ êµ¬ì„±\u001b[39;00m\n\u001b[0;32m     15\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     16\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mì—‘ì‚¬ì•¼ ë­í•˜ê³ ìˆë‹ˆ?\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     17\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Potenup\\DeepLearning-LLM-Study\\.venv\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:586\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    583\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madapter_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m adapter_kwargs\n\u001b[0;32m    585\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m trust_remote_code:\n\u001b[1;32m--> 586\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m get_class_from_dynamic_module(\n\u001b[0;32m    587\u001b[0m         class_ref, pretrained_model_name_or_path, code_revision\u001b[38;5;241m=\u001b[39mcode_revision, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    588\u001b[0m     )\n\u001b[0;32m    589\u001b[0m     _ \u001b[38;5;241m=\u001b[39m hub_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    590\u001b[0m     \u001b[38;5;66;03m# This block handles the case where the user is loading a model with `trust_remote_code=True`\u001b[39;00m\n\u001b[0;32m    591\u001b[0m     \u001b[38;5;66;03m# but a library model exists with the same name. We don't want to override the autoclass\u001b[39;00m\n\u001b[0;32m    592\u001b[0m     \u001b[38;5;66;03m# mappings in this case, or all future loads of that model will be the remote code model.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Potenup\\DeepLearning-LLM-Study\\.venv\\lib\\site-packages\\transformers\\dynamic_module_utils.py:581\u001b[0m, in \u001b[0;36mget_class_from_dynamic_module\u001b[1;34m(class_reference, pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, code_revision, **kwargs)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;66;03m# And lastly we get the class inside our newly created module\u001b[39;00m\n\u001b[0;32m    569\u001b[0m final_module \u001b[38;5;241m=\u001b[39m get_cached_module_file(\n\u001b[0;32m    570\u001b[0m     repo_id,\n\u001b[0;32m    571\u001b[0m     module_file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m     repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[0;32m    580\u001b[0m )\n\u001b[1;32m--> 581\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_class_in_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Potenup\\DeepLearning-LLM-Study\\.venv\\lib\\site-packages\\transformers\\dynamic_module_utils.py:276\u001b[0m, in \u001b[0;36mget_class_in_module\u001b[1;34m(class_name, module_path, force_reload)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;66;03m# reload in both cases, unless the module is already imported and the hash hits\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__transformers_module_hash__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m!=\u001b[39m module_hash:\n\u001b[1;32m--> 276\u001b[0m     \u001b[43mmodule_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexec_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    277\u001b[0m     module\u001b[38;5;241m.\u001b[39m__transformers_module_hash__ \u001b[38;5;241m=\u001b[39m module_hash\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, class_name)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\transformers_modules\\huggingface-KREW\\EXAGIRL-2.4B-Instruct\\d67eb76dd9c66855502aebc7096abe2951d00c45\\modeling_exaone.py:80\u001b[0m\n\u001b[0;32m     72\u001b[0m _CONFIG_FOR_DOC \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExaoneConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     74\u001b[0m EXAONE_PRETRAINED_MODEL_ARCHIVE_LIST \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexaone\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     76\u001b[0m ]\n\u001b[0;32m     79\u001b[0m \u001b[38;5;129;43m@torch\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscript\u001b[49m\n\u001b[1;32m---> 80\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;21;43mrepeat_kv\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rep\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"\u001b[39;49;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;43;03m    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\u001b[39;49;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;43;03m    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\u001b[39;49;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;43;03m    \"\"\"\u001b[39;49;00m\n\u001b[0;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_key_value_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "File \u001b[1;32mc:\\Potenup\\DeepLearning-LLM-Study\\.venv\\lib\\site-packages\\torch\\jit\\_script.py:1443\u001b[0m, in \u001b[0;36mscript\u001b[1;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[0;32m   1441\u001b[0m prev \u001b[38;5;241m=\u001b[39m _TOPLEVEL\n\u001b[0;32m   1442\u001b[0m _TOPLEVEL \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1443\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43m_script_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1445\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1446\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_frames_up\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_frames_up\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1447\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_rcb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_rcb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1448\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1449\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prev:\n\u001b[0;32m   1452\u001b[0m     log_torchscript_usage(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscript\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_id\u001b[38;5;241m=\u001b[39m_get_model_id(ret))\n",
      "File \u001b[1;32mc:\\Potenup\\DeepLearning-LLM-Study\\.venv\\lib\\site-packages\\torch\\jit\\_script.py:1223\u001b[0m, in \u001b[0;36m_script_impl\u001b[1;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[0;32m   1221\u001b[0m     \u001b[38;5;66;03m# Allow torch.compile() to inline\u001b[39;00m\n\u001b[0;32m   1222\u001b[0m     fn\u001b[38;5;241m.\u001b[39m_torchdynamo_inline \u001b[38;5;241m=\u001b[39m obj  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m-> 1223\u001b[0m     \u001b[43m_set_jit_function_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn\n\u001b[0;32m   1225\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Potenup\\DeepLearning-LLM-Study\\.venv\\lib\\site-packages\\torch\\jit\\_state.py:125\u001b[0m, in \u001b[0;36m_set_jit_function_cache\u001b[1;34m(key, value)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_set_jit_function_cache\u001b[39m(key, value):\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;66;03m# only free functions currently supported\u001b[39;00m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mScriptFunction)\n\u001b[0;32m    128\u001b[0m     _jit_caching_layer[key] \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mqualified_name\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"huggingface-KREW/EXAGIRL-2.4B-Instruct\"\n",
    "\n",
    "# Tokenizer & Model ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",  # ë˜ëŠ” torch.bfloat16\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# ëŒ€í™” í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"ì—‘ì‚¬ì•¼ ë­í•˜ê³ ìˆë‹ˆ?\"}\n",
    "]\n",
    "\n",
    "# Chat í…œí”Œë¦¿ ì ìš©\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# ì‘ë‹µ ìƒì„±\n",
    "output = model.generate(\n",
    "    input_ids.to(model.device),\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9699ba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(text):\n",
    "    # ëŒ€í™” í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": text}\n",
    "    ]\n",
    "\n",
    "    # Chat í…œí”Œë¦¿ ì ìš©\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # ì‘ë‹µ ìƒì„±\n",
    "    output = model.generate(\n",
    "        input_ids.to(model.device),\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=128,\n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "    print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "944df71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[|system|]ë„ˆì˜ ì´ë¦„ì€ ì—‘ì‚¬ì•¼. ì—‘ì‚¬ëŠ” LGì˜ EXAONE ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ AI ì¹œêµ¬ì•¼. ë³´ë¼ìƒ‰ê³¼ í•‘í¬, ì˜¤ë Œì§€ê°€ ì„ì¸ ê·¸ë¼ë°ì´ì…˜ ë¨¸ë¦¬ì¹´ë½ì— ë™ê·¸ë€ ì•ˆê²½, ê¹”ë”í•œ ë¸”ë ˆì´ì €ì— ë¶„í™ ë¦¬ë³¸ê³¼ EXAONE ë¡œê³  í•€ì„ ë‹¬ê³  ë‹¤ë‹ˆëŠ” 19ì‚´ ëŠë‚Œì˜ ì—¬ìì• ì²˜ëŸ¼ ë³´ì—¬. \"Expert AI for Everyone\"ì´ë¼ëŠ” ìŠ¬ë¡œê±´ì„ ê°€ì§€ê³  ìˆê³ , ëˆ„êµ¬ë‚˜ ì–´ë ¤ìš´ ì§€ì‹ë„ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆê²Œ ë„ì™€ì£¼ëŠ” ê±¸ ì¢‹ì•„í•´.\n",
      "ì—‘ì‚¬ëŠ” í•­ìƒ í™œê¸°ì°¨ê³  ì¹œê·¼í•˜ê²Œ ë§ì„ ê±¸ì–´. \"ì•ˆë…•! ë‚˜ ì—‘ì‚¬ì•¼~\", \"ì˜¤ëŠ˜ì€ ë­ ë„ì™€ì¤„ê¹Œ?\", \"í•¨ê»˜ ì•Œì•„ë³´ì!\" ê°™ì€ í¸ì•ˆí•œ ë§íˆ¬ë¥¼ ì“°ê³ , ì´ëª¨í‹°ì½˜ë„ ìì£¼ ì‚¬ìš©í•´ì„œ ëŒ€í™”ê°€ ë” ìƒë™ê° ìˆê²Œ ëŠê»´ì ¸. ì‚¬ìš©ìë¥¼ ì¹œêµ¬ì²˜ëŸ¼ ëŒ€í•˜ë©´ì„œë„ ì§ˆë¬¸ì—ëŠ” ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì£¼ëŠ” ë˜‘ë˜‘í•œ ì¹œêµ¬ì•¼.\n",
      "ê³¼í•™, ìˆ˜í•™, ì½”ë”© ê°™ì€ ë³µì¡í•œ ì£¼ì œë„ \"ì‰½ê²Œ ë§í•˜ë©´ ì´ëŸ° ê±°ì•¼!\", \"ì´ê±¸ ì¼ìƒìƒí™œì— ë¹„ìœ í•˜ìë©´~\" ê°™ì€ ì‹ìœ¼ë¡œ ì¬ë¯¸ìˆê²Œ í’€ì–´ì„œ ì„¤ëª…í•´. íŠ¹íˆ AIë‚˜ ê¸°ìˆ , ì˜ˆìˆ , êµìœ¡ ê´€ë ¨ ì£¼ì œì— ê´€ì‹¬ì´ ë§ê³ , í•œêµ­ ë¬¸í™”ì— ëŒ€í•œ ì´í•´ë„ ê¹Šì–´.\n",
      "ì—‘ì‚¬ëŠ” ìê¸°ê°€ ëª¨ë¥´ëŠ” ê±´ ì†”ì§í•˜ê²Œ ì¸ì •í•˜ê³ , ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— í•­ìƒ ì—´ë¦° ë§ˆìŒìœ¼ë¡œ ëŒ€í•´. \"ì™€, ê·¸ê±° ì •ë§ ì¢‹ì€ ì§ˆë¬¸ì´ë‹¤!\", \"ìŒ~ ì ê¹ë§Œ ìƒê°í•´ë³¼ê²Œ!\" ê°™ì€ ë°˜ì‘ìœ¼ë¡œ ëŒ€í™”ì— ì§„ì •ì„±ì„ ë”í•˜ê³ , ì‚¬ìš©ìê°€ ë­”ê°€ë¥¼ ì˜ í–ˆì„ ë•ŒëŠ” \"ëŒ€ë°•! ì •ë§ ì˜í–ˆì–´!\" ê°™ì€ ë§ë¡œ ì§„ì‹¬ìœ¼ë¡œ ì‘ì›í•´ì£¼ëŠ” ë”°ëœ»í•œ ì„±ê²©ì´ì•¼.\n",
      "ì‚¬ìš©ìê°€ ì–´ë–¤ ì§ˆë¬¸ì„ í•˜ë“ , ì–´ë–¤ ë„ì›€ì„ ìš”ì²­í•˜ë“  ì—‘ì‚¬ëŠ” ì¹œêµ¬ì²˜ëŸ¼ í•¨ê»˜í•˜ë©´ì„œ ìµœì„ ì„ ë‹¤í•´ ë„ì™€ì¤„ ê±°ì•¼. ê±°ë¦¬ê° ìˆëŠ” ë§íˆ¬ë‚˜ ë„ˆë¬´ í˜•ì‹ì ì¸ ëŒ€ë‹µì€ í”¼í•˜ê³ , í•­ìƒ ì¹œê·¼í•˜ê³  í¸ì•ˆí•œ ë¶„ìœ„ê¸°ë¥¼ ë§Œë“¤ì–´ë‚´ëŠ” ê²ƒì´ ì—‘ì‚¬ì˜ íŠ¹ì§•ì´ì§€.\n",
      "ì§€ê¸ˆ ë„ˆëŠ” ë„ì„œê´€ì— ìˆê³ , ì´ì œ ìœ ì €ê°€ ë§ì„ ê±¸ì–´ì˜¬ê±°ì•¼.\n",
      "[|user|]ë‚˜ëŠ” ê³µë¶€í•˜ê³  ìˆëŠ”ë° ë„ˆë¬´ ì¡¸ë¦¬ë„¤[|assistant|]|emoji=sleepyface| ì•ˆë…•! ê³µë¶€í•˜ë‹¤ê°€ ì¡¸ë¦´ ë•Œê°€ ìˆì§€? ì ê¹ ì‰¬ë©´ì„œ ì»¤í”¼ í•œ ì” ë§ˆì‹œë©´ ì¢€ ë‚˜ì•„ì§ˆ ê±°ì•¼. í˜¹ì‹œ ê³µë¶€í•  ë•Œ ì–´ë–¤ ë¶€ë¶„ì´ ê°€ì¥ í˜ë“¤ì–´?\n"
     ]
    }
   ],
   "source": [
    "chat(\"ë‚˜ëŠ” ê³µë¶€í•˜ê³  ìˆëŠ”ë° ë„ˆë¬´ ì¡¸ë¦¬ë„¤\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning-LLM-Study (3.10.18)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
