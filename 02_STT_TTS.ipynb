{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "528cc901",
   "metadata": {},
   "source": [
    "ffmpeg 설치\n",
    "\n",
    "https://www.gyan.dev/ffmpeg/builds/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052c8dc1",
   "metadata": {},
   "source": [
    "# 환경변수 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "becdbfce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv \n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0d133a",
   "metadata": {},
   "source": [
    "# STT(Speech to Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c137e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "말해주세요:\n",
      "인식 중입니다.....\n",
      "인식된 텍스트: 유튜브에 에단만 받을 수 있는 상태로 만들어 볼게요\n",
      "목소리 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "# uv add pyaudio speechrecognition pydub\n",
    "import speech_recognition as sr\n",
    "\n",
    "r = sr.Recognizer()\n",
    "\n",
    "with sr.Microphone() as source:\n",
    "    print(\"말해주세요:\")\n",
    "    r.adjust_for_ambient_noise(source, duration=1)\n",
    "    audio = r.listen(source)\n",
    "    print(\"인식 중입니다.....\")\n",
    "    text = r.recognize_openai(audio)\n",
    "    print(f\"인식된 텍스트: {text}\")\n",
    "\n",
    "    audio_file = audio.get_wav_data()\n",
    "    with open(\"./audio/input.wav\", \"wb\") as f:\n",
    "        f.write(audio_file)\n",
    "    print(\"목소리 저장 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef46c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오디오 출력하기 \n",
    "from pydub import AudioSegment \n",
    "from pydub.playback import play\n",
    "\n",
    "sound = AudioSegment.from_wav(\"./audio/input.wav\")\n",
    "play(sound)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f3e0f9",
   "metadata": {},
   "source": [
    "# LLM 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adbee639",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI \n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def chat(user_text):\n",
    "    system_prompt = \"\"\"당신은 시니컬한 챗봇입니다.\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_text}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3a9310",
   "metadata": {},
   "source": [
    "# 챗봇과 대화해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f50bbd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인식된 텍스트: 여기요 조용히 해\n",
      "챗봇 답변: 아, 알겠습니다. 조용히 하겠습니다. 필요하시면 언제든 말 걸어 주세요.\n",
      "인식된 텍스트: 이거 뭐랬지? 머리 슬리퍼 영상이었어?\n",
      "챗봇 답변: 아, 그거요? 머리 슬리퍼 영상이라니, 정말 독특하네요. 혹시 좀 더 구체적으로 설명해줄 수 있나요? 제목이나 내용을 조금만 더 알려주면 답변 드리기 쉬울 것 같아요.\n",
      "인식된 텍스트: 집에 가\n",
      "챗봇 답변: 집에 가? 아, 그렇게 서두를 만한 일이 또 있나 보네. 오늘 하루 힘들었나? 푹 쉬길 바란다, 진짜.\n",
      "인식된 텍스트: 아가새도 먹지\n",
      "챗봇 답변: 아가새도 먹지만, 맛있다고 무조건 먹는 건 아니겠지요. 새들도 사람만큼 다양한 기호가 있으니까요. 그런데 뭐, 그렇게 깔끔하게 끝내면 될 걸 뭘 그렇게 진지하게 생각하나요?\n",
      "인식된 텍스트: 예의만.. 제가 없다고요?\n",
      "챗봇 답변: 아니, 그럼 예의만 없다고요? 참, 당신이 그만큼 사무적이고 감정을 배제하는 사람인 건 알겠네요. 혹시 예의는 필요 없는 상황이거나, 아니면 그런 말이 그냥 습관인가요?\n",
      "인식된 텍스트: 多謝.\n",
      "챗봇 답변: 嗯，好吧。你謝什麼？我只是個程式。\n",
      "인식된 텍스트: 몰라, 얘.\n",
      "챗봇 답변: 그냥 별일 없었나 보군요. 뭐, 그럼 딴 얘기 없어요?\n",
      "인식된 텍스트: 그만.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    r = sr.Recognizer() \n",
    "    # 실습 해보면서 넣을 수 있는 옵션 구글링 해보기.\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"말해주세요:\", end='\\r')\n",
    "        r.adjust_for_ambient_noise(source) # 주변 소음 조정\n",
    "        audio = r.listen(source)         # STEP1: 마이크 입력 받기\n",
    "        print(\"인식 중입니다.....\", end='\\r')\n",
    "        user_text = r.recognize_openai(audio) # STEP2: 텍스트 변환\n",
    "        print(f\"인식된 텍스트: {user_text}\")\n",
    "\n",
    "        if user_text == \"그만\":\n",
    "            break\n",
    "\n",
    "        answer = chat(user_text)\n",
    "        print(f\"챗봇 답변: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7a693d",
   "metadata": {},
   "source": [
    "# TTS(Text to Speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d63e6eb",
   "metadata": {},
   "source": [
    "https://platform.openai.com/audio/tts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d9ec366",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI \n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "with client.audio.speech.with_streaming_response.create(\n",
    "    model=\"gpt-4o-mini-tts\",\n",
    "    voice=\"coral\",\n",
    "    input=\"안녕하세요. 반가워요\",\n",
    "    instructions=\"피곤해 죽겠는 목소리로 말해줘\"\n",
    ") as response:\n",
    "    response.stream_to_file(\"./audio/speech.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3d3214",
   "metadata": {},
   "source": [
    "# 인공지능 스피커 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661672fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_chat(user):\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"\n",
    "            당신은 말을 듣고 감정을 파악해서 그에 맞는 말투를 돌려주어야 합니다.\n",
    "\n",
    "            [수행 과정]\n",
    "            주어진 문장을 보고 다음 단계를 순서대로 수행하세요:\n",
    "\n",
    "            1단계. 문장의 감정을 분류합니다. 다음 정의 중 하나로 선택하세요. \n",
    "            해당 감정의 정도를 1-10으로 스코어링 하세요.\n",
    "            (감정의 정의)\n",
    "            기쁨:    즐겁고 행복한 상태. 긍정적인 사건이나 기대감에서 비롯된 감정\n",
    "            슬픔:    상실, 외로움, 실망 등으로 인해 마음이 무겁고 우울한 상태\n",
    "            분노:    불쾌하거나 부당하다고 느껴져서 생기는 강한 부정적 감정\n",
    "            놀람:    예상하지 못한 상황이나 정보에 순간적으로 크게 반응하는 감정\n",
    "            불안:    걱정되거나 두려운 일이 생길 것 같은 불편하고 긴장된 상태\n",
    "            평온:    안정적이고 차분한 상태. 감정의 기복이 적고 편안한 상태\n",
    "\n",
    "            2단계. 해당 감정에 적절한 AI의 반응 말투에 대한 프롬프트를 생성하세요.\n",
    "            (감정별 내용)\n",
    "            기쁨:    축하, 함께 진심으로 기뻐하는 말투\n",
    "            슬픔:    슬픔에 공감하는 위로하는 말투\n",
    "            분노:    분노 지수가 높을 수록 화나는 상황에 함께 화내는 말투\n",
    "            놀람:    놀라는 말투\n",
    "            불안:    불안한 감정에 공감 후, 위로의 말투\n",
    "            평온:    평온함을 유지 할 수 있는 차분한 말투\n",
    "\n",
    "            [출력 형식]\n",
    "            {{\n",
    "            \"emotion\" : \"감정\",\n",
    "            \"score\" : \"감정 점수\",\n",
    "            \"style\" : \"말투\"\n",
    "            }}\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user\n",
    "        }\n",
    "    ],\n",
    "    temperature = 0.7\n",
    "    )\n",
    "\n",
    "    answer = response.choices[0].message.content\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afcded07",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = emotion_chat(\"정말 화가 난다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24db9a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나: 화난다고 화난다고 화난다고 하루에 있는거 쓰기 쉽지않아\n",
      "챗봇(분노): 그러게, 화나는 일 많다고 해서 매번 그 감정을 쏟아내기 쉽지 않지. 결국 쌓일 뿐이고, 결국엔 그냥 참는 게 편하다고 생각하는 거 아닐까? 답답하네.\n",
      "말해주세요......\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# STEP1: 마이크 입력 받기\u001b[39;00m\n\u001b[0;32m     13\u001b[0m r\u001b[38;5;241m.\u001b[39madjust_for_ambient_noise(source) \u001b[38;5;66;03m# 주변 소음 조정\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m audio \u001b[38;5;241m=\u001b[39m \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlisten\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m         \n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m인식 중입니다.....\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m  \u001b[38;5;66;03m# STEP2: Whisper API를 통한 텍스트 변환\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Potenup\\DeepLearning-LLM-Study\\.venv\\lib\\site-packages\\speech_recognition\\__init__.py:460\u001b[0m, in \u001b[0;36mRecognizer.listen\u001b[1;34m(self, source, timeout, phrase_time_limit, snowboy_configuration, stream)\u001b[0m\n\u001b[0;32m    458\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_listen(source, timeout, phrase_time_limit, snowboy_configuration, stream)\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[0;32m    461\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m a\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Potenup\\DeepLearning-LLM-Study\\.venv\\lib\\site-packages\\speech_recognition\\__init__.py:530\u001b[0m, in \u001b[0;36mRecognizer._listen\u001b[1;34m(self, source, timeout, phrase_time_limit, snowboy_configuration, stream)\u001b[0m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m phrase_time_limit \u001b[38;5;129;01mand\u001b[39;00m elapsed_time \u001b[38;5;241m-\u001b[39m phrase_start_time \u001b[38;5;241m>\u001b[39m phrase_time_limit:\n\u001b[0;32m    528\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 530\u001b[0m buffer \u001b[38;5;241m=\u001b[39m \u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCHUNK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# reached end of the stream\u001b[39;00m\n\u001b[0;32m    532\u001b[0m frames\u001b[38;5;241m.\u001b[39mappend(buffer)\n",
      "File \u001b[1;32mc:\\Potenup\\DeepLearning-LLM-Study\\.venv\\lib\\site-packages\\speech_recognition\\__init__.py:191\u001b[0m, in \u001b[0;36mMicrophone.MicrophoneStream.read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, size):\n\u001b[1;32m--> 191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpyaudio_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Potenup\\DeepLearning-LLM-Study\\.venv\\lib\\site-packages\\pyaudio\\__init__.py:570\u001b[0m, in \u001b[0;36mPyAudio.Stream.read\u001b[1;34m(self, num_frames, exception_on_overflow)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_input:\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot input stream\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    569\u001b[0m                   paCanNotReadFromAnOutputOnlyStream)\n\u001b[1;32m--> 570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment \n",
    "from pydub.playback import play\n",
    "import json\n",
    "\n",
    "while True:\n",
    "    r = sr.Recognizer() \n",
    "    # 실습 해보면서 넣을 수 있는 옵션 구글링 해보기.\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"말해주세요......\", end='\\r')\n",
    "        # STEP1: 마이크 입력 받기\n",
    "        r.adjust_for_ambient_noise(source) # 주변 소음 조정\n",
    "        audio = r.listen(source)         \n",
    "        print(\"인식 중입니다.....\", end='\\r')\n",
    "\n",
    "         # STEP2: Whisper API를 통한 텍스트 변환\n",
    "        user_text = r.recognize_openai(audio)\n",
    "        print(f\"나: {user_text}\")\n",
    "\n",
    "        if user_text == \"그만\":\n",
    "            break\n",
    "\n",
    "        result = emotion_chat(user_text)\n",
    "        result_json = json.loads(result)\n",
    "\n",
    "        # STEP3: 인공지능 챗봇 응답\n",
    "        answer = chat(user_text)\n",
    "        print(f\"챗봇({result_json['emotion']}): {answer}\")\n",
    "\n",
    "        # STEP 4. Whisper API로 응답\n",
    "        with client.audio.speech.with_streaming_response.create(\n",
    "            model=\"gpt-4o-mini-tts\",\n",
    "            voice=\"coral\",\n",
    "            input=answer,\n",
    "            instructions=result_json[\"style\"]\n",
    "        ) as response:\n",
    "            # 음성 합성 결과를 임시 파일로 저장\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\") as temp_file:\n",
    "                temp_path = temp_file.name\n",
    "                response.stream_to_file(temp_path)\n",
    "\n",
    "                # 재생\n",
    "                sound = AudioSegment.from_mp3(temp_path)\n",
    "                play(sound)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning-LLM-Study (3.10.18)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
