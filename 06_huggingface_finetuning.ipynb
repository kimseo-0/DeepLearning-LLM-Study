{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50211033",
   "metadata": {},
   "source": [
    "# LLM 모델 파인튜닝하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6796d880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경변수 불러오기\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# huggingface 로그인\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "login(token=os.getenv(\"HF_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d239de8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu126\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f844d96e",
   "metadata": {},
   "source": [
    "# 1. 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c8544e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dd91845061f413c9e91dc7f92026c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "model_name = \"google/gemma-2b-it\"\n",
    "\n",
    "# 8비트(LoRA + int8) —— 정확도 보존↑, 메모리 절약은 중간(≈2~3배)\n",
    "## VRAM 12GB에서 안정/보수적으로 돌리고 싶을 때 기본 선택\n",
    "## 추론 품질 손실을 최소화하고 싶을 때도 선호\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,          # ✅ 가중치를 8bit로 로드(메모리 절감, 정확도 손실 적음)\n",
    "    load_in_4bit=False,         # 4bit 경로 비활성화(헷갈림 방지)\n",
    "\n",
    "    # llm_int8_threshold:\n",
    "    #  - outlier(이상치) 채널을 감지해 해당 채널만 고정밀 경로로 우회하는 기준.\n",
    "    #  - 기본 6.0이 널리 사용. 숫자를 낮추면 우회 채널이 늘어 정밀↑(메모리/속도↓).\n",
    "    llm_int8_threshold=6.0,\n",
    "\n",
    "    # llm_int8_has_fp16_weight:\n",
    "    #  - 일부 레이어(예: outlier가 많은 레이어)의 가중치를 fp16로 유지할지 여부.\n",
    "    #  - False면 전반적으로 8bit 경로에 머무름(메모리↓). True면 더 보수적(정확도↑, 메모리↑).\n",
    "    llm_int8_has_fp16_weight=False,\n",
    ")\n",
    "\n",
    "# 4비트(QLoRA) —— 메모리 절약 극대화(≈4~5배), 대형 모델/대량 배치에 유리\n",
    "## 4070 12GB에서 최대한 큰 모델/배치를 돌리고 싶을 때 선택\n",
    "## 약간의 정밀도 손실 감수 가능할 때\n",
    "# quantization_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,          # 가중치를 4bit로 로드(QLoRA 핵심, VRAM 절약 극대화)\n",
    "#     load_in_8bit=False,         # 8bit 경로 비활성화\n",
    "\n",
    "#     # 연산 정밀도(계산 dtype):\n",
    "#     # - 가중치는 4bit로 ‘보관’하지만, 실제 곱셈/합산은 16비트로 수행.\n",
    "#     # - 4070(아다러너)라면 bfloat16 권장(underflow에 강하고 안정적).\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16,  # 또는 torch.float16 (환경에 따라 테스트)\n",
    "\n",
    "#     # Double Quantization:\n",
    "#     # - 1차 4bit 양자화에 쓰인 스케일/제로포인트 자체를 다시 압축.\n",
    "#     # - 메모리 추가 절감 효과. 보통 품질 영향은 미미해 ON 권장.\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "\n",
    "#     # 4bit 포맷:\n",
    "#     # - \"nf4\": NormalFloat4. LLM 분포에 맞춘 4bit 포맷으로 QLoRA 기본 추천.\n",
    "#     # - \"fp4\": Float4. 연구/특수 목적이 아니라면 nf4가 일반적으로 더 좋음.\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a755b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 양자화 > 추론용, 작은 모델을 만들 경우 활용\n",
    "# paameter : quantization_config=BitsAndBytesConfig()\n",
    "# BitsAndByte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f24cc19",
   "metadata": {},
   "source": [
    "# 2. 토크나이저 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f1cc30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트가 있을 때 텐서로 잘 바꿔주는지 확인하기\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "475baa40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [2, 238179, 243415, 204551], 'attention_mask': [1, 1, 1, 1]}\n",
      "안녕하세요\n"
     ]
    }
   ],
   "source": [
    "text = \"안녕하세요\"\n",
    "tokenzied_messages = tokenizer(text)\n",
    "print(tokenzied_messages)\n",
    "decoded_text = tokenizer.decode(tokenzied_messages['input_ids'], skip_special_tokens=True)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31d7a5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[     2,    106,   1645,    108, 240471, 240046, 101969, 235248, 241305,\n",
      "         239042, 237138, 237014,  96673,    108,  70685, 243415, 204551,    107,\n",
      "            108,    106,   2516,    108, 238179, 243415, 204551, 235265,  78821,\n",
      "         236361, 239779, 237526,    107,    108]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "user\n",
      "친절하게 답변해주세요\n",
      " 안녕하세요\n",
      "model\n",
      "안녕하세요. 반가워요\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"친절하게 답변해주세요\\n 안녕하세요\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"안녕하세요. 반가워요\"\n",
    "        },\n",
    "    ],\n",
    "]\n",
    "tokenzied_messages = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=False,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "print(tokenzied_messages)\n",
    "\n",
    "decoded_text = tokenizer.decode(\n",
    "    tokenzied_messages['input_ids'][0],\n",
    "    skip_special_tokens = True\n",
    "    )\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f5376d",
   "metadata": {},
   "source": [
    "# 3. 학습 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb23ed7",
   "metadata": {},
   "source": [
    "## 1) Huggingface에서 제공하는 데이터셋 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48202f34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['date', 'category', 'press', 'title', 'document', 'link', 'summary'],\n",
       "        num_rows: 22194\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['date', 'category', 'press', 'title', 'document', 'link', 'summary'],\n",
       "        num_rows: 2466\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['date', 'category', 'press', 'title', 'document', 'link', 'summary'],\n",
       "        num_rows: 2740\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "dataset = load_dataset(\"daekeun-ml/naver-news-summarization-ko\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72187884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['date', 'category', 'press', 'title', 'document', 'link', 'summary'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"].select(range(10)) # 10개만 가져오기\n",
    "dataset[\"train\"].shuffle(seed=42).select(range(10)) # 랜덤으로 섞은 뒤 10개만 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23f483db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = dataset['train'].select(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37ac2ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date': '2022-07-03 17:14:37',\n",
       " 'category': 'economy',\n",
       " 'press': 'YTN ',\n",
       " 'title': '추경호 중기 수출지원 총력 무역금융 40조 확대',\n",
       " 'document': '앵커 정부가 올해 하반기 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 했습니다. 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했습니다. 류환홍 기자가 보도합니다. 기자 수출은 최고의 실적을 보였지만 수입액이 급증하면서 올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록했습니다. 정부가 수출확대에 총력을 기울이기로 한 것은 원자재 가격 상승 등 대외 리스크가 가중되는 상황에서 수출 증가세 지속이야말로 한국경제의 회복을 위한 열쇠라고 본 것입니다. 추경호 경제부총리 겸 기획재정부 장관 정부는 우리 경제의 성장엔진인 수출이 높은 증가세를 지속할 수 있도록 총력을 다하겠습니다. 우선 물류 부담 증가 원자재 가격 상승 등 가중되고 있는 대외 리스크에 대해 적극 대응하겠습니다. 특히 중소기업과 중견기업 수출 지원을 위해 무역금융 규모를 연초 목표보다 40조 원 늘린 301조 원까지 확대하고 물류비 부담을 줄이기 위한 대책도 마련했습니다. 이창양 산업통상자원부 장관 국제 해상운임이 안정될 때까지 월 4척 이상의 임시선박을 지속 투입하는 한편 중소기업 전용 선복 적재 용량 도 현재보다 주당 50TEU 늘려 공급하겠습니다. 하반기에 우리 기업들의 수출 기회를 늘리기 위해 2 500여 개 수출기업을 대상으로 해외 전시회 참가를 지원하는 등 마케팅 지원도 벌이기로 했습니다. 정부는 또 이달 중으로 반도체를 비롯한 첨단 산업 육성 전략을 마련해 수출 증가세를 뒷받침하고 에너지 소비를 줄이기 위한 효율화 방안을 마련해 무역수지 개선에 나서기로 했습니다. YTN 류환홍입니다.',\n",
       " 'link': 'https://n.news.naver.com/mnews/article/052/0001759333?sid=101',\n",
       " 'summary': '올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록한 가운데, 정부가 하반기에 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 결정한 가운데, 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했다.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c4a7cc",
   "metadata": {},
   "source": [
    "## 2) 커스텀 데이터 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ae69a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instruct 모델을 파인튜닝하는 이유는? > 다음 신문을 요약해주세요. 그리고 핵심 키워드 5개를 뽑아주세요\n",
    "# STEP1. 위에서 불러온 데이터를 가지고 Keywords를 뽑아주는 데이터를 만들어야 한다.\n",
    "# STEP2. document, summary, keywords를 가지고 messages 형태의 데이터를 만든다.\n",
    "# messages = [\n",
    "#     [\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": \n",
    "#         },\n",
    "#         {\n",
    "#             \"role\": \"assistant\",\n",
    "#             \"content\": \n",
    "#         },\n",
    "#     ],\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca7b54f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP1. 위에서 불러온 데이터를 가지고 Keywords를 뽑아주는 데이터를 만들어야 한다.\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "# data = {'date': '2022-07-03 17:14:37',\n",
    "#  'category': 'economy',\n",
    "#  'press': 'YTN ',\n",
    "#  'title': '추경호 중기 수출지원 총력 무역금융 40조 확대',\n",
    "#  'document': '앵커 정부가 올해 하반기 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 했습니다. 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했습니다. 류환홍 기자가 보도합니다. 기자 수출은 최고의 실적을 보였지만 수입액이 급증하면서 올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록했습니다. 정부가 수출확대에 총력을 기울이기로 한 것은 원자재 가격 상승 등 대외 리스크가 가중되는 상황에서 수출 증가세 지속이야말로 한국경제의 회복을 위한 열쇠라고 본 것입니다. 추경호 경제부총리 겸 기획재정부 장관 정부는 우리 경제의 성장엔진인 수출이 높은 증가세를 지속할 수 있도록 총력을 다하겠습니다. 우선 물류 부담 증가 원자재 가격 상승 등 가중되고 있는 대외 리스크에 대해 적극 대응하겠습니다. 특히 중소기업과 중견기업 수출 지원을 위해 무역금융 규모를 연초 목표보다 40조 원 늘린 301조 원까지 확대하고 물류비 부담을 줄이기 위한 대책도 마련했습니다. 이창양 산업통상자원부 장관 국제 해상운임이 안정될 때까지 월 4척 이상의 임시선박을 지속 투입하는 한편 중소기업 전용 선복 적재 용량 도 현재보다 주당 50TEU 늘려 공급하겠습니다. 하반기에 우리 기업들의 수출 기회를 늘리기 위해 2 500여 개 수출기업을 대상으로 해외 전시회 참가를 지원하는 등 마케팅 지원도 벌이기로 했습니다. 정부는 또 이달 중으로 반도체를 비롯한 첨단 산업 육성 전략을 마련해 수출 증가세를 뒷받침하고 에너지 소비를 줄이기 위한 효율화 방안을 마련해 무역수지 개선에 나서기로 했습니다. YTN 류환홍입니다.',\n",
    "#  'link': 'https://n.news.naver.com/mnews/article/052/0001759333?sid=101',\n",
    "#  'summary': '올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록한 가운데, 정부가 하반기에 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 결정한 가운데, 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했다.'\n",
    "#  }\n",
    "\n",
    "def get_keywords(data): \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"뉴스에서 핵심 키워드를 예시와 같이 명사로 5개 추출해주세요\\n\\n[뉴스전문] {data['document']}\\n\\n[예시] 키워드1, 키워드2, 키워드3, 키워드4, 키워드5\"\n",
    "        }\n",
    "    ]\n",
    "    # output = pipe(messages, max_new_tokens=128)\n",
    "    output = client.chat.completions.create(model=\"gpt-5-nano\", messages=messages)\n",
    "    output = output.choices[0].message.content\n",
    "\n",
    "    data[\"keywords\"] = output\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81f58704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date': '2022-07-03 17:14:37',\n",
       " 'category': 'economy',\n",
       " 'press': 'YTN ',\n",
       " 'title': '추경호 중기 수출지원 총력 무역금융 40조 확대',\n",
       " 'document': '앵커 정부가 올해 하반기 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 했습니다. 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했습니다. 류환홍 기자가 보도합니다. 기자 수출은 최고의 실적을 보였지만 수입액이 급증하면서 올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록했습니다. 정부가 수출확대에 총력을 기울이기로 한 것은 원자재 가격 상승 등 대외 리스크가 가중되는 상황에서 수출 증가세 지속이야말로 한국경제의 회복을 위한 열쇠라고 본 것입니다. 추경호 경제부총리 겸 기획재정부 장관 정부는 우리 경제의 성장엔진인 수출이 높은 증가세를 지속할 수 있도록 총력을 다하겠습니다. 우선 물류 부담 증가 원자재 가격 상승 등 가중되고 있는 대외 리스크에 대해 적극 대응하겠습니다. 특히 중소기업과 중견기업 수출 지원을 위해 무역금융 규모를 연초 목표보다 40조 원 늘린 301조 원까지 확대하고 물류비 부담을 줄이기 위한 대책도 마련했습니다. 이창양 산업통상자원부 장관 국제 해상운임이 안정될 때까지 월 4척 이상의 임시선박을 지속 투입하는 한편 중소기업 전용 선복 적재 용량 도 현재보다 주당 50TEU 늘려 공급하겠습니다. 하반기에 우리 기업들의 수출 기회를 늘리기 위해 2 500여 개 수출기업을 대상으로 해외 전시회 참가를 지원하는 등 마케팅 지원도 벌이기로 했습니다. 정부는 또 이달 중으로 반도체를 비롯한 첨단 산업 육성 전략을 마련해 수출 증가세를 뒷받침하고 에너지 소비를 줄이기 위한 효율화 방안을 마련해 무역수지 개선에 나서기로 했습니다. YTN 류환홍입니다.',\n",
       " 'link': 'https://n.news.naver.com/mnews/article/052/0001759333?sid=101',\n",
       " 'summary': '올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록한 가운데, 정부가 하반기에 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 결정한 가운데, 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했다.',\n",
       " 'keywords': '수출, 무역금융, 물류비, 임시선박, 해외전시회'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_keywords(sample_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd88ba46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function get_keywords at 0x0000018F16CCF5B0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b44b69fd369e46e28795beadb34f6b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_dataset = sample_dataset.map(get_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51d7c579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['date', 'category', 'press', 'title', 'document', 'link', 'summary', 'keywords'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f6c6530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP2. document, summary, keywords를 가지고 messages 형태의 데이터를 만든다.\n",
    "\n",
    "def make_prompt(data):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"뉴스를 요약해주세요.\\n\\n 그리고 핵심 키워드를 예시와 같이 명사로 5개 추출해주세요\\n\\n[뉴스전문] {data['document']}\\n\\n[예시]키워드: 키워드1, 키워드2, 키워드3, 키워드4, 키워드5\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": f\"[키워드]: {data['keywords']}\\n\\n[뉴스요약]: {data['summary']}\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"text\": tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=False,\n",
    "            tokenize=False\n",
    "        ) + \"<eos>\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "801ca376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '<bos><start_of_turn>user\\n뉴스를 요약해주세요.\\n\\n 그리고 핵심 키워드를 예시와 같이 명사로 5개 추출해주세요\\n\\n[뉴스전문] 앵커 정부가 올해 하반기 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 했습니다. 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했습니다. 류환홍 기자가 보도합니다. 기자 수출은 최고의 실적을 보였지만 수입액이 급증하면서 올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록했습니다. 정부가 수출확대에 총력을 기울이기로 한 것은 원자재 가격 상승 등 대외 리스크가 가중되는 상황에서 수출 증가세 지속이야말로 한국경제의 회복을 위한 열쇠라고 본 것입니다. 추경호 경제부총리 겸 기획재정부 장관 정부는 우리 경제의 성장엔진인 수출이 높은 증가세를 지속할 수 있도록 총력을 다하겠습니다. 우선 물류 부담 증가 원자재 가격 상승 등 가중되고 있는 대외 리스크에 대해 적극 대응하겠습니다. 특히 중소기업과 중견기업 수출 지원을 위해 무역금융 규모를 연초 목표보다 40조 원 늘린 301조 원까지 확대하고 물류비 부담을 줄이기 위한 대책도 마련했습니다. 이창양 산업통상자원부 장관 국제 해상운임이 안정될 때까지 월 4척 이상의 임시선박을 지속 투입하는 한편 중소기업 전용 선복 적재 용량 도 현재보다 주당 50TEU 늘려 공급하겠습니다. 하반기에 우리 기업들의 수출 기회를 늘리기 위해 2 500여 개 수출기업을 대상으로 해외 전시회 참가를 지원하는 등 마케팅 지원도 벌이기로 했습니다. 정부는 또 이달 중으로 반도체를 비롯한 첨단 산업 육성 전략을 마련해 수출 증가세를 뒷받침하고 에너지 소비를 줄이기 위한 효율화 방안을 마련해 무역수지 개선에 나서기로 했습니다. YTN 류환홍입니다.\\n\\n[예시]키워드: 키워드1, 키워드2, 키워드3, 키워드4, 키워드5<end_of_turn>\\n<start_of_turn>model\\n[키워드]: 수출, 무역금융, 물류비, 임시선박, 무역수지\\n\\n[뉴스요약]: 올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록한 가운데, 정부가 하반기에 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 결정한 가운데, 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했다.<end_of_turn>\\n<eos>'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_prompt(sample_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10b9ab26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2b0b8e2556a4460a2835f48a106b940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_dataset = sample_dataset.map(make_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359774a3",
   "metadata": {},
   "source": [
    "## 3) 학습데이터 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5aec08ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages = [\n",
    "#     [\n",
    "#         {\n",
    "#             \"role\": \"system\",\n",
    "#             \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},]\n",
    "#         },\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": [{\"type\": \"text\", \"text\": \"Write a poem on Hugging Face, the company\"},]\n",
    "#         },\n",
    "#     ],\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d3d1355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[\"train\"].map(func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bfce10",
   "metadata": {},
   "source": [
    "# 4. 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6fe8fb",
   "metadata": {},
   "source": [
    "## 1) 학습 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a683948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFTConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1975dee8",
   "metadata": {},
   "source": [
    "## LoraConfig 주요 파라미터 정리\n",
    "\n",
    "| 파라미터 | 의미 | 권장 기본값 | 커스텀 가이드 |\n",
    "|----------|------|-------------|----------------|\n",
    "| r | LoRA 어댑터의 차원(랭크) | 16 | VRAM 적으면 8, 성능↑ 원하면 32 |\n",
    "| lora_alpha | LoRA 스케일링 계수 (실질 영향력은 alpha/r) | 32 (보통 r*2) | 성능 부족 → 크게, 안정성 필요 → 작게 |\n",
    "| lora_dropout | 어댑터 입력에 드롭아웃 적용 (학습 시만) | 0.05 | 데이터 적고 과적합 우려 → 0.1 ↑, 데이터 충분 → 0 |\n",
    "| bias | bias 파라미터 포함 여부 | \"none\" | 거의 항상 \"none\" 사용 |\n",
    "| task_type | 적용할 태스크 유형 | \"CAUSAL_LM\" | 생성형 LM이면 그대로 사용 |\n",
    "| target_modules | LoRA를 적용할 레이어 이름 패턴 | [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"] | 시작은 어텐션만 → 필요시 FFN까지(gate/up/down_proj) 확대 |\n",
    "| modules_to_save | LoRA 외에 저장할 모듈 | None | 추가 성능 필요 시 [\"lm_head\"] 같이 지정 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b00a43a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 가중치를 업데이트 하는 방식 : Full FT\n",
    "# 가중치 일부만 바꾸는 방식 : PEFT(Parameter-Effecient FT) \n",
    "#   LoRA(저차원행렬의 곱셈) 기존의 가중치는 공정하고 Adapter의 가중치를 업데이터한다.\n",
    "from peft import LoraConfig\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r = 6,\n",
    "    lora_alpha = 8,\n",
    "    lora_dropout = 0.05,\n",
    "    target_modules = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7109bda",
   "metadata": {},
   "source": [
    "## 2) 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7521213",
   "metadata": {},
   "source": [
    "## SFTConfig 주요 파라미터 (SFT 전용)\n",
    "| 파라미터 | 설명 | 부연 설명 |\n",
    "|----------|------|----------|\n",
    "| output_dir | 체크포인트 저장 경로 | TrainingArguments와 동일 |\n",
    "| num_train_epochs | 학습 epoch 수 | 동일 |\n",
    "| per_device_train_batch_size | GPU당 학습 배치 크기 | 동일 |\n",
    "| gradient_accumulation_steps | 기울기 누적 스텝 | 동일 |\n",
    "| learning_rate | 학습률 | LoRA/QLoRA일 경우 full FT보다 높게 설정하는 경우 많음 |\n",
    "| weight_decay | 가중치 감소 | 동일 |\n",
    "| lr_scheduler_type | 학습률 스케줄러 | 동일 |\n",
    "| warmup_ratio / warmup_steps | 워밍업 | 동일 |\n",
    "| logging_steps | 로그 기록 주기 | 동일 |\n",
    "| save_steps | 체크포인트 저장 주기 | 동일 |\n",
    "| bf16 / fp16 | 혼합정밀도 학습 | 동일 |\n",
    "| max_seq_length | 최대 입력 길이 | 한 문장이 이 길이를 넘으면 잘림 |\n",
    "| packing | 여러 샘플 묶기 | True면 작은 문장 여러 개를 block_size에 합쳐 효율↑ |\n",
    "| dataset_text_field | 텍스트 열 이름 | 어떤 열을 학습 텍스트로 쓸지 지정 |\n",
    "| peft_config | LoRA/QLoRA 설정 | 효율적 미세튜닝 시 필요 |\n",
    "| optim | 옵티마이저 | QLoRA 시 `paged_adamw_8bit` 자주 사용 |\n",
    "| gradient_checkpointing | 메모리 절약 | 동일 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "796c5f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir = \"./outputs\",\n",
    "    num_train_epochs=2,\n",
    "    max_steps=2,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f179d6",
   "metadata": {},
   "source": [
    "## SFTTrainer 주요 파라미터\n",
    "| 파라미터 | 설명 | 부연 설명 |\n",
    "|----------|------|----------|\n",
    "| model | 학습할 모델 | 동일 |\n",
    "| tokenizer | 토크나이저 | 동일 |\n",
    "| train_dataset | 학습 데이터셋 | 동일 |\n",
    "| eval_dataset | 검증 데이터셋 | 동일 |\n",
    "| args | SFTConfig 객체 | TrainingArguments 대신 SFTConfig 사용 |\n",
    "| formatting_func | 데이터 포맷팅 함수 | 원시 데이터 → 대화형 텍스트 변환 (ex. instruction/output) |\n",
    "| dataset_text_field | 텍스트 열 이름 | SFTConfig와 연결됨 |\n",
    "| max_seq_length | 최대 입력 길이 | 동일 |\n",
    "| packing | 여러 샘플 묶기 | 동일 |\n",
    "| peft_config | LoRA/QLoRA 설정 | 동일 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80fc3480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Potenup\\DeepLearning-LLM-Study\\.venv\\lib\\site-packages\\peft\\mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "c:\\Potenup\\DeepLearning-LLM-Study\\.venv\\lib\\site-packages\\peft\\tuners\\tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73a8eb356729436692b3bf1a81583e7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39058e60ffa74e1380b470813d45714c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a11077bde1034e8a86a4c43a6681952c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# uv add trl\n",
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=sample_dataset,\n",
    "    args=args,\n",
    "    peft_config=lora_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c1e487",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Potenup\\DeepLearning-LLM-Study\\.venv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c44e4be",
   "metadata": {},
   "source": [
    "## 3) 학습된 모델 저장"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning-LLM-Study (3.10.18)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
